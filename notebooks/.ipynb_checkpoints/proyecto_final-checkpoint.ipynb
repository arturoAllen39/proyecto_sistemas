{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36761ab-27e0-436e-9364-d5d78c9de6aa",
   "metadata": {},
   "source": [
    "# Aqui inicia mi sufrimiento :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0734abf6-fdf5-41bb-897d-7e830b7a703f",
   "metadata": {},
   "source": [
    "# PROYECTO FINAL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f105ba-bc9e-4ecc-8cc6-31f48eb3973d",
   "metadata": {},
   "source": [
    "## Sistemas Distribuidos con PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6529ba-82b3-4d45-ad34-217567980814",
   "metadata": {},
   "source": [
    "Este proyecto implementa lo que se ha visto en clases en una serie de ejercicios a las cuales llamo un **sistema de recomendaci√≥n de libros** utilizando:\n",
    "- **100 libros** m√°s descargados de Project Gutenberg (Omitiendo 2 ya que estos denegaban el acceso...)\n",
    "- **PySpark** para procesamiento distribuido\n",
    "- **TF-IDF** para an√°lisis de texto\n",
    "- **Similitud coseno** para encontrar libros similares\n",
    "- Y las ense√±anzas del profe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ab2e8-c827-4e56-8630-6e32b7433b30",
   "metadata": {},
   "source": [
    "## CELDA 1: Configuraci√≥n de Rutas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1dc7cd-9fd2-4c4b-ad2f-5d65dcde75cd",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\n",
    "Configura las rutas del proyecto para que Python pueda encontrar nuestros archivos de utilidades (`src/utils.py`).\n",
    "\n",
    "**Explicaci√≥n t√©cnica:**\n",
    "- `sys.path.append()` a√±ade directorios donde Python buscar√° m√≥dulos <details> <summary> ¬øPor que? </summary> (Antes tuvimos problemas con que el programa que no encontraba la ruta de SRC)</details>\n",
    "\n",
    "\n",
    "- `..` = directorio padre (ra√≠z del proyecto)\n",
    "- `../src` = carpeta de c√≥digo fuente<details>\n",
    "    <summary> ¬øQue hay ahi? </summary> \n",
    "    Ahi adentro se encuentran los archivos python donde uno descarga los libros (download_books.py) y otro donde se encarga de limpiar estos mismos como quitar los headers o footers para que no extorben en el analisis (utils.py)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433a5e47-9932-438f-a8bf-4394212638ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rutas configuradas:\n",
      "  - /home/arturoallen/proyecto_52_sistemas/lib/python3.12/site-packages\n",
      "  - /home/arturoallen/proyecto_52_sistemas/proyecto_sistemas\n",
      "  - /home/arturoallen/proyecto_52_sistemas/proyecto_sistemas/src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Agregar rutas del proyecto\n",
    "sys.path.append(os.path.abspath(\"..\"))  # ra√≠z del proyecto\n",
    "sys.path.append(os.path.abspath(\"../src\"))  # carpeta src\n",
    "\n",
    "# Verificar rutas\n",
    "print(\"Rutas configuradas:\")\n",
    "for path in sys.path[-3:]:\n",
    "    print(f\"  - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6cbe4-d551-414c-b1cd-d83fe5084d05",
   "metadata": {},
   "source": [
    "## CELDA 2: Descargar Recursos de NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78bc74a-185b-458b-bae2-0cec356ead26",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\n",
    "Descarga las herramientas necesarias de NLTK para procesar texto en ingl√©s.\n",
    "\n",
    "**Lo que descarga:**\n",
    "- `punkt` y `punkt_tab` ‚Üí Separa texto en palabras (tokenizaci√≥n)<details><summary>En otras palabras</summary>Esto divide un texto grande en piezas m√°s peque√±as llamadas tokens.</details>\n",
    "\n",
    "- `stopwords` ‚Üí Lista de palabras comunes que no aportan significado (\"the\", \"a\", \"is\") <details>\n",
    "  <summary> ¬øDe donde sacamos esta herramienta? </summary>\n",
    "  Esta herramienta o mas bien libreria fue recomendada por el profesor para facilitarnos la detecci√≤n de stopwords\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70bf0cce-277a-4c2d-9279-eda87ff104ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando recursos de NLTK...\n",
      "‚úì NLTK configurado\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(\"Descargando recursos de NLTK...\")\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"‚úì NLTK configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9da3f-8de6-4cd3-a39d-0c26d6526d31",
   "metadata": {},
   "source": [
    "## CELDA 3: Inicializar Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ae35a-8dd2-4c83-b2b9-6ad323492441",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\n",
    "Arranca el motor de Spark que procesar√° nuestros datos de forma distribuida.\n",
    "\n",
    "**Configuraci√≥n:**\n",
    "- `driver.memory: 6g` ‚Üí Memoria del coordinador (6 GB)<details><summary>Mas detalles</summary>Memoria para el driver.\n",
    "    - Este driver.memory recibe las instrucciones, reparte el trabajo y al final recoge los resultados.\n",
    "    - Los 6 GB significa que la m√°quina virtual va a reservar 6 GB solo para este coordinador.\n",
    "    - Esto procesara los 100 libros\n",
    "</details>\n",
    "\n",
    "- `executor.memory: 4g` ‚Üí Memoria de los trabajadores (4 GB)\n",
    "- `local[*]` ‚Üí Usa todos los cores de tu CPU\n",
    "\n",
    "\\\n",
    "**¬øPor qu√© Spark?**\n",
    "- Para procesar 100 libros (~75 MB) de forma eficiente y paralela.<details>\n",
    "    <summary> Y... </summary> \n",
    "    ...porque el profe nos lo pidio\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c8eca0-8361-4af2-bd81-0e7727d96a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/10 19:01:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/10 19:01:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Spark inicializado\n",
      "  Version: 4.0.1\n",
      "  Master: local[2]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProyectoFinal_RecomendacionLibros\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reducir logs\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úì Spark inicializado\")\n",
    "print(f\"  Version: {spark.version}\")\n",
    "print(f\"  Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b85d2313-1fcb-4619-9227-8a5f50c345c1",
   "metadata": {},
   "source": [
    "## CELDA 4: Importar Funciones de Utilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69880f-56d6-48b3-ac38-117eeadca922",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**¬øQu√© hace?**\\\n",
    "Carga las funciones que creamos para procesar los libros desde la carpeta src al archivo utils.py\n",
    "\n",
    "**Funciones importadas:**\n",
    "- `read_txt()` ‚Üí Lee un archivo .txt sin importar el tipo de encoding, evitando errores por acentos o caracteres raros.\n",
    "- `strip_gutenberg_headers()` ‚Üí Quita las licencias, advertencias y texto extra que agregan los libros.\n",
    "- `preprocess_text()` ‚Üí Limpia el texto aplicando estos filtros:\n",
    "    - min√∫sculas\n",
    "    - tokenizar\n",
    "    - quitar stopwords\n",
    "    - quitar puntuaci√≥n<br>\n",
    "- `load_all_books()` ‚Üí Busca todos los .txt de la carpeta y:\\\n",
    "  Es la funci√≥n que carga todos los libros del proyecto.\n",
    "    - los lee\n",
    "    - los limpia\n",
    "    - los convierte en una lista/diccionario de libros procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b18b06b-0ec8-4475-9598-02b1bde9a6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Utilidades importadas\n"
     ]
    }
   ],
   "source": [
    "from src.utils import (\n",
    "    read_txt,\n",
    "    strip_gutenberg_headers,\n",
    "    preprocess_text,\n",
    "    load_all_books\n",
    ")\n",
    "\n",
    "print(\"‚úì Utilidades importadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da20a93-d253-4d3b-988d-65a24569b75e",
   "metadata": {},
   "source": [
    "## CELDA 5 y 6: Cargar 100 Libros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2af80-4949-415e-8b79-2ea4dd9db477",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\n",
    "Lee los 100 lirbos .txt desde la carpeta `data/` y los preprocesa.\n",
    "\n",
    "**¬øEn que consiste el proceso?**\n",
    "1. Lee el archivo\n",
    "2. Elimina headers/footers\n",
    "3. Convierte a min√∫sculas\n",
    "4. Separa en palabras (tokens)\n",
    "5. Elimina stopwords (\"the\", \"a\", etc.)\n",
    "6. Elimina puntuaci√≥n y n√∫meros\n",
    "\n",
    "**Resultado:**\n",
    "Una lista con 100 libros, cada uno con:\n",
    "- ID del libro (usando los ID originales de la pagina)\n",
    "- Nombre del archivo\n",
    "- Texto completo\n",
    "- Lista de palabras limpias (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b046493d-9de7-48ae-905f-d889e8f27cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desmarcar el comentario en caso de extrema emergencia (No tener los libros)\n",
    "#!python3 ../src/download_books.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5548773-4fb8-4791-8945-456cf428e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Cargando 100 libros desde ../data/\n",
      "  ‚úì Procesados 10/100 libros\n",
      "  ‚úì Procesados 20/100 libros\n",
      "  ‚úì Procesados 30/100 libros\n",
      "  ‚úì Procesados 40/100 libros\n",
      "  ‚úì Procesados 50/100 libros\n",
      "  ‚úì Procesados 60/100 libros\n",
      "  ‚úì Procesados 70/100 libros\n",
      "  ‚úì Procesados 80/100 libros\n",
      "  ‚úì Procesados 90/100 libros\n",
      "  ‚úì Procesados 100/100 libros\n",
      "‚úÖ Total de libros cargados exitosamente: 100\n",
      "\n",
      "\n",
      " Resumen de carga:\n",
      "   Total de libros: 100\n",
      "   Ejemplo - ID: A Christmas Carol - Charles Dickens, Tokens: 13396\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"\n",
    "books = load_all_books(data_dir, max_books=100)\n",
    "\n",
    "print(f\"\\n Resumen de carga:\")\n",
    "print(f\"   Total de libros: {len(books)}\")\n",
    "if books:\n",
    "    ejemplo = books[0]\n",
    "    print(f\"   Ejemplo - ID: {ejemplo[0]}, Tokens: {len(ejemplo[3])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e19a27-a5bf-4c65-8776-cb644ba39a9c",
   "metadata": {},
   "source": [
    "## CELDA 7: Crear DataFrame de Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a187e94-7db3-46c0-9c2f-01aa1fb5d8ff",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\n",
    "Convierte nuestra lista de libros en un **DataFrame de Spark** (como una especie de tabla de Excel gigante).\n",
    "\n",
    "**Columnas del DataFrame:**\n",
    "- `book_id` ‚Üí ID √∫nico del libro (ej: \"84\", \"1342\")\n",
    "- `title` ‚Üí Nombre del archivo (ej: \"84.txt\")\n",
    "- `text` ‚Üí Texto completo del libro\n",
    "- `tokens` ‚Üí Lista de palabras procesadas\n",
    "\n",
    "**¬øPor qu√© un DataFrame?**\n",
    "Spark puede procesar DataFrames de forma paralela y distribuida.\n",
    "<details><summary>Comentario</summary> Creemos que no se ve las columnas de \"text\" y \"tokens\" porque estamos usando una maquina virutal para correr este codigo y la verdad a este punto en el que estoy hubiera preferido instalar linux pero en la laptop, casi no la uso</details>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1f561-be48-42d5-8a81-ece277d9b2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando DataFrame de Spark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/10 19:02:51 WARN TaskSetManager: Stage 0 contains a task of very large size (29816 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 2) / 2]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "print(\"Creando DataFrame de Spark...\")\n",
    "df = spark.createDataFrame(\n",
    "    [(b[0], b[1], b[3]) for b in books],\n",
    "    schema=[\"book_id\", \"title\", \"tokens\"]\n",
    ")\n",
    "print(f\"‚úì DataFrame creado con {df.count()} documentos\\n\")\n",
    "df.select(\"book_id\", \"title\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7075463e-47af-47da-b461-7c92cfad75c6",
   "metadata": {},
   "source": [
    "## CELDA 8: Crear Vocabulario (CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c0a3d-f909-49de-bd0a-36601592ee25",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\\\n",
    "Crea un **vocabulario** con las 2000 palabras m√°s importantes de todos los libros.\n",
    "\n",
    "**Proceso:**\n",
    "1. Cuenta cu√°ntas veces aparece cada palabra en cada libro\n",
    "2. Selecciona las 2000 palabras m√°s frecuentes\n",
    "3. Filtra palabras que aparecen en menos de 2 libros (muy raras)\n",
    "\n",
    "**Resultado:**\\\n",
    "Cada libro se representa como un vector de 2000 n√∫meros (frecuencias de palabras).\n",
    "<details>\n",
    "  <summary>Detalles del codigo</summary>\n",
    "  <p>Par√°metros:</p>\n",
    "  <ul>\n",
    "    <li>VOCAB_SIZE=2000: limita el vocabulario a las 2000 palabras m√°s frecuentes. Evita vectores enormes.</li>\n",
    "    <li>MIN_DF=2: elimina palabras que aparecen en menos de 2 documentos (filtra ruido / typos).</li>\n",
    "  </ul>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebbb1b6-e009-429c-9371-f078e3b843d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "print(\"Creando vocabulario con CountVectorizer...\")\n",
    "\n",
    "VOCAB_SIZE = 2000\n",
    "MIN_DF = 2\n",
    "\n",
    "cv = CountVectorizer(\n",
    "    inputCol=\"tokens\",\n",
    "    outputCol=\"raw_features\",\n",
    "    vocabSize=VOCAB_SIZE,\n",
    "    minDF=MIN_DF\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(df)\n",
    "df = cv_model.transform(df)\n",
    "df = df.drop(\"tokens\")  \n",
    "\n",
    "actual_vocab_size = len(cv_model.vocabulary)\n",
    "print(f\"‚úì Vocabulario creado\")\n",
    "print(f\"  Tama√±o del vocabulario: {actual_vocab_size} palabras\")\n",
    "print(f\"  Top 10 palabras: {cv_model.vocabulary[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6e2eb-bce7-47e7-9456-a16c03c94e98",
   "metadata": {},
   "source": [
    "## CELDA 9: Calcular TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a4951-3252-4492-99dd-a72887e6b499",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\\\n",
    "Calcula **TF-IDF** (Term Frequency - Inverse Document Frequency) para cada palabra.\n",
    "\n",
    "**¬øQu√© es TF-IDF?**\\\n",
    "Un n√∫mero que indica el **peso** o **importancia** en una palabra para un libro espec√≠fico.\n",
    "\n",
    "**F√≥rmula simple:**\n",
    "- **TF** (frecuencia): ¬øCu√°ntas veces aparece en ESTE libro?\n",
    "- **IDF** (rareza): ¬øQu√© tan rara es en TODOS los libros?\n",
    "- **TF-IDF** = TF √ó IDF\n",
    "\n",
    "**Ejemplo:**\n",
    "- \"elizabeth\" aparece mucho en Frankenstein ‚Üí TF alto\n",
    "- \"elizabeth\" NO aparece en otros libros ‚Üí IDF alto\n",
    "- **TF-IDF de \"elizabeth\"** = ALTO (palabra caracter√≠stica)\n",
    "\n",
    "**¬øAl final que se obtiene de todo esto?:**\\\n",
    "Cada libro tiene un vector TF-IDF que representa su contenido √∫nico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3486077-f274-439a-a567-977f14d59f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "print(\"\\nCalculando TF-IDF manualmente con la f√≥rmula...\")\n",
    "print(\"F√≥rmula: TF-IDF(d,t) = TF(d,t) √ó log(N / DF(t))\")\n",
    "\n",
    "# Recolectar datos de raw_features (salida de CountVectorizer)\n",
    "rows = df.select(\"book_id\", \"raw_features\").collect()\n",
    "\n",
    "N = len(rows)  # Total de documentos (100)\n",
    "vocab_size = len(cv_model.vocabulary)  # Tama√±o del vocabulario (2000)\n",
    "\n",
    "print(f\"  ‚Üí N = {N} documentos\")\n",
    "print(f\"  ‚Üí Vocabulario = {vocab_size} palabras\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1: Calcular DF (Document Frequency) para cada palabra\n",
    "# ============================================================================\n",
    "print(\"\\n  PASO 1: Contando en cu√°ntos documentos aparece cada palabra...\")\n",
    "\n",
    "# df[i] = n√∫mero de documentos que contienen la palabra i\n",
    "df_array = np.zeros(vocab_size)\n",
    "\n",
    "for row in rows:\n",
    "    vector = row[\"raw_features\"]\n",
    "    # Contar palabras √∫nicas en este documento\n",
    "    for idx in vector.indices:\n",
    "        df_array[idx] += 1\n",
    "\n",
    "print(f\"     ‚úì DF calculado para {vocab_size} palabras\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 2: Calcular IDF para cada palabra\n",
    "# ============================================================================\n",
    "print(\"\\n  PASO 2: Calculando IDF = log(N / DF) para cada palabra...\")\n",
    "\n",
    "# idf[i] = log(N / df[i])\n",
    "idf_array = np.zeros(vocab_size)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    if df_array[i] > 0:\n",
    "        # F√≥rmula: IDF(t) = log(N / DF(t))\n",
    "        idf_array[i] = math.log(N / df_array[i])\n",
    "    else:\n",
    "        idf_array[i] = 0.0\n",
    "\n",
    "print(f\"     ‚úì IDF calculado\")\n",
    "print(f\"     ‚úì Rango IDF: [{idf_array.min():.4f}, {idf_array.max():.4f}]\")\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"\\n     Ejemplos de IDF:\")\n",
    "for i in range(min(5, len(cv_model.vocabulary))):\n",
    "    word = cv_model.vocabulary[i]\n",
    "    df_val = df_array[i]\n",
    "    idf_val = idf_array[i]\n",
    "    print(f\"       '{word}': DF={int(df_val)} docs ‚Üí IDF = log({N}/{int(df_val)}) = {idf_val:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 3: Calcular TF-IDF = TF √ó IDF para cada documento\n",
    "# ============================================================================\n",
    "print(\"\\n  PASO 3: Multiplicando TF √ó IDF para cada documento...\")\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "tfidf_results = []\n",
    "\n",
    "for doc_idx, row in enumerate(rows):\n",
    "    book_id = row[\"book_id\"]\n",
    "    tf_vector = row[\"raw_features\"]\n",
    "    \n",
    "    # Calcular TF-IDF para palabras no-cero\n",
    "    indices = []\n",
    "    values = []\n",
    "    \n",
    "    for i, idx in enumerate(tf_vector.indices):\n",
    "        tf_value = tf_vector.values[i]\n",
    "        idf_value = idf_array[idx]\n",
    "        \n",
    "        # TF-IDF = TF √ó IDF\n",
    "        tfidf_value = tf_value * idf_value\n",
    "        \n",
    "        indices.append(int(idx))\n",
    "        values.append(float(tfidf_value))\n",
    "    \n",
    "    # Crear vector disperso\n",
    "    tfidf_sparse = Vectors.sparse(vocab_size, indices, values)\n",
    "    tfidf_results.append((book_id, tfidf_sparse))\n",
    "    \n",
    "    if (doc_idx + 1) % 20 == 0:\n",
    "        print(f\"     ‚Üí Procesados {doc_idx + 1}/{N} documentos...\")\n",
    "\n",
    "print(f\"     ‚úì TF-IDF calculado para {N} documentos\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 4: Crear DataFrame con TF-IDF\n",
    "# ============================================================================\n",
    "print(\"\\n  PASO 4: Creando DataFrame con vectores TF-IDF...\")\n",
    "\n",
    "from pyspark.sql import Row\n",
    "tfidf_rows = [Row(book_id=r[0], tfidf=r[1]) for r in tfidf_results]\n",
    "tfidf_df = spark.createDataFrame(tfidf_rows)\n",
    "\n",
    "# Unir con DataFrame original\n",
    "df = df.join(tfidf_df, on=\"book_id\", how=\"inner\")\n",
    "df = df.drop(\"raw_features\")\n",
    "\n",
    "print(\"‚úì TF-IDF calculado con f√≥rmula expl√≠cita\")\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICACI√ìN: Mostrar c√°lculo detallado para un libro\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VERIFICACI√ìN: C√°lculo detallado para el primer libro\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ejemplo = rows[0]\n",
    "ejemplo_id = ejemplo[\"book_id\"]\n",
    "ejemplo_tf = ejemplo[\"raw_features\"]\n",
    "\n",
    "print(f\"\\nLibro: {ejemplo_id}\")\n",
    "print(f\"\\nTop 5 palabras con mayor TF-IDF:\\n\")\n",
    "\n",
    "# Calcular TF-IDF para este libro\n",
    "palabra_tfidf = []\n",
    "for i, idx in enumerate(ejemplo_tf.indices):\n",
    "    word = cv_model.vocabulary[idx]\n",
    "    tf = ejemplo_tf.values[i]\n",
    "    idf = idf_array[idx]\n",
    "    tfidf = tf * idf\n",
    "    palabra_tfidf.append((word, tf, idf, tfidf))\n",
    "\n",
    "# Ordenar por TF-IDF\n",
    "palabra_tfidf.sort(key=lambda x: x[3], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "#for i, (word, tf, idf, tfidf) in enumerate(palabra_tfidf[:5], 1):\n",
    "#    print(f\"{i}. '{word}':\")\n",
    "#    print(f\"   TF = {tf:.0f} (apariciones en el libro)\")\n",
    "#    print(f\"   IDF = {idf:.4f} (rareza en la colecci√≥n)\")\n",
    "#    print(f\"   TF-IDF = {tf:.0f} √ó {idf:.4f} = {tfidf:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27740ee8-562e-4785-94b5-1ff086403a3f",
   "metadata": {},
   "source": [
    "## CELDA 10: Normalizar Vectores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c83a3-e2eb-4d6e-8fa5-ee7a21dc322d",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\\\n",
    "Normaliza los vectores TF-IDF para que todos tengan la misma \"longitud\" matem√°tica.<details> <summary>Ejemplo</summary> Es como comparar la composici√≥n de dos bebidas sin importar el tama√±o del vaso.</details>\n",
    "\n",
    "**¬øPor qu√© normalizar?**\n",
    "- Libros largos tienen n√∫meros m√°s grandes\n",
    "- Queremos comparar **proporcionalmente**, no por tama√±o\n",
    "- Despu√©s de normalizar, los valores est√°n entre 0 y 1\n",
    "\n",
    "\n",
    "**Resultado:**\\\n",
    "Vectores `tfidf_norm` con valores entre 0 y 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7eb27d-3444-424a-b8ad-84529f3f522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "print(\"Normalizando vectores TF-IDF...\")\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"tfidf_norm\", p=2.0)\n",
    "df = normalizer.transform(df)\n",
    "df = df.drop(\"tfidf\")\n",
    "\n",
    "print(\"‚úì Vectores normalizados\")\n",
    "#print(\"\\nDataFrame final:\")\n",
    "#df.select(\"book_id\", \"title\").show(7, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3fb2fa-be44-44b0-85ce-4d5a4270fa31",
   "metadata": {},
   "source": [
    "## CELDA 11: Crear Matriz de Similitud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d714ca8-b9a4-46f0-a2b4-6f3553a5e239",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\n",
    "Crea una **matriz 100√ó100** que compara cada libro con todos los dem√°s.\n",
    "\n",
    "**Proceso:**\n",
    "1. Convierte vectores de Spark a NumPy (arrays de Python)\n",
    "2. Calcula el **producto punto** entre todos los pares de libros\n",
    "3. El resultado es la **similitud coseno**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f57e3b-1357-4a11-8d18-bb6e305964da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"Creando matriz de similitud...\")\n",
    "\n",
    "# Recolectar datos\n",
    "rows = df.select(\"book_id\", \"title\", \"tfidf_norm\").collect()\n",
    "\n",
    "# Preparar estructuras de datos\n",
    "book_ids = [r[\"book_id\"] for r in rows]\n",
    "id_to_title = {r[\"book_id\"]: r[\"title\"] for r in rows}\n",
    "\n",
    "# Crear √≠ndices num√©ricos\n",
    "numeric_id_to_original = {i+1: book_ids[i] for i in range(len(book_ids))}\n",
    "original_to_numeric_id = {book_ids[i]: i+1 for i in range(len(book_ids))}\n",
    "\n",
    "# Convertir vectores a numpy\n",
    "print(\"  ‚Üí Convirtiendo vectores a numpy...\")\n",
    "vectors = np.array([r[\"tfidf_norm\"].toArray() for r in rows])\n",
    "\n",
    "# Calcular matriz de similitud CON LA F√ìRMULA COMPLETA DESDE CERO\n",
    "print(\"  ‚Üí Calculando similitudes con f√≥rmula: cos(Œ∏) = (u¬∑v) / (||u|| √ó ||v||)\")\n",
    "\n",
    "n_books = vectors.shape[0]  # N√∫mero de libros\n",
    "n_features = vectors.shape[1]  # Dimensiones del vector TF-IDF\n",
    "\n",
    "sim_matrix = np.zeros((n_books, n_books))\n",
    "\n",
    "for i in range(n_books):\n",
    "    for j in range(n_books):\n",
    "        # PASO 1: Calcular producto punto manualmente (u ¬∑ v)\n",
    "        # u¬∑v = u‚ÇÅ√óv‚ÇÅ + u‚ÇÇ√óv‚ÇÇ + ... + u‚Çô√óv‚Çô\n",
    "        dot_product = 0.0\n",
    "        for k in range(n_features):\n",
    "            dot_product += vectors[i][k] * vectors[j][k]\n",
    "        \n",
    "        # PASO 2: Calcular magnitud de u: ||u|| = ‚àö(u‚ÇÅ¬≤ + u‚ÇÇ¬≤ + ... + u‚Çô¬≤)\n",
    "        norm_i = 0.0\n",
    "        for k in range(n_features):\n",
    "            norm_i += vectors[i][k] ** 2\n",
    "        norm_i = np.sqrt(norm_i)\n",
    "        \n",
    "        # PASO 3: Calcular magnitud de v: ||v|| = ‚àö(v‚ÇÅ¬≤ + v‚ÇÇ¬≤ + ... + v‚Çô¬≤)\n",
    "        norm_j = 0.0\n",
    "        for k in range(n_features):\n",
    "            norm_j += vectors[j][k] ** 2\n",
    "        norm_j = np.sqrt(norm_j)\n",
    "        \n",
    "        # PASO 4: Similitud del coseno = (u¬∑v) / (||u|| √ó ||v||)\n",
    "        if norm_i > 0 and norm_j > 0:  # Evitar divisi√≥n por cero\n",
    "            sim_matrix[i][j] = dot_product / (norm_i * norm_j)\n",
    "        else:\n",
    "            sim_matrix[i][j] = 0.0\n",
    "    \n",
    "    # Mostrar progreso cada 100 libros\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"     Procesados {i + 1}/{n_books} libros...\")\n",
    "\n",
    "# Crear mapeos (mantener los originales tambi√©n)\n",
    "index_to_id = {i: book_ids[i] for i in range(len(book_ids))}\n",
    "id_to_index = {book_ids[i]: i for i in range(len(book_ids))}\n",
    "\n",
    "print(f\"‚úì Matriz de similitud creada: {sim_matrix.shape}\")\n",
    "print(f\"  Rango de similitudes: [{sim_matrix.min():.4f}, {sim_matrix.max():.4f}]\")\n",
    "print(f\"  üìä {len(numeric_id_to_original)} libros indexados num√©ricamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ec04e-f535-4033-a75d-7495a80c4d2b",
   "metadata": {},
   "source": [
    "##  CELDA 12: Recomendar libros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f5e8c0-0931-438c-be2f-b77e9689f014",
   "metadata": {},
   "source": [
    "\n",
    "**¬øQu√© hace?**\\\n",
    "Ingresamos un libro, seguido de ello se encuentra los N libros m√°s similares (Por default y para evitar que la memoria muera pusimos 5).\n",
    "\n",
    "**Pasos:**\n",
    "1. Busca el libro en la matriz de similitud\n",
    "2. Obtiene sus similitudes con todos los dem√°s libros\n",
    "3. Ordena de mayor a menor similitud\n",
    "4. Retorna los top N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d6174-1846-44ea-bc9e-1bb6126ce3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendar_libros(libro_id, N=5):\n",
    "    # Convertir a string para comparaci√≥n\n",
    "    libro_id_str = str(libro_id)\n",
    "    \n",
    "    # Intentar con ID num√©rico primero\n",
    "    if libro_id_str.isdigit():\n",
    "        num_id = int(libro_id_str)\n",
    "        if num_id in numeric_id_to_original:\n",
    "            libro_id_str = numeric_id_to_original[num_id]\n",
    "            print(f\"üìñ Usando ID num√©rico {num_id} ‚Üí {libro_id_str}\")\n",
    "    \n",
    "    # Validar que existe\n",
    "    if libro_id_str not in id_to_index:\n",
    "        raise ValueError(f\"Libro '{libro_id}' no encontrado\")\n",
    "        \n",
    "    idx = id_to_index[libro_id_str]\n",
    "    similarities = sim_matrix[idx]\n",
    "\n",
    "    pairs = [\n",
    "        (index_to_id[i], float(similarities[i]))\n",
    "        for i in range(len(similarities))\n",
    "        if i != idx\n",
    "    ]\n",
    "\n",
    "    pairs_sorted = sorted(pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # gregar ID num√©rico a los resultados\n",
    "    results = [\n",
    "        (original_to_numeric_id[bid], bid, id_to_title[bid], score)\n",
    "        for bid, score in pairs_sorted[:N]\n",
    "    ]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35f962-315d-4ed9-bc54-c9fa0d4aa2e4",
   "metadata": {},
   "source": [
    "## CELDA 13: Funci√≥n Palabras Importantes (NORMALIZADA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db84348-ebf1-409e-92c2-61b49f4bb6d8",
   "metadata": {},
   "source": [
    "\n",
    "**¬øQu√© hace?**\\\n",
    "Encuentra las M palabras m√°s caracter√≠sticas de un libro.\n",
    "\n",
    "**Proceso:**\n",
    "1. Obtiene el vector TF-IDF normalizado del libro\n",
    "2. Ordena las palabras por su score TF-IDF\n",
    "3. Retorna las top M palabras\n",
    "\n",
    "**¬øPor qu√© usa `tfidf_norm`?**\n",
    "Los valores est√°n entre 0 y 1, m√°s f√°ciles de interpretar como porcentajes.\n",
    "\n",
    "**Entrada:**\n",
    "- `libro_id` = \"84\"\n",
    "- `M` = 5\n",
    "\n",
    "**Salida:**\n",
    "```\n",
    "elizabeth ‚Üí 0.64 (64% de importancia)\n",
    "feelings  ‚Üí 0.15 (15% de importancia)\n",
    "henry     ‚Üí 0.12 (12% de importancia)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6bdb0-6247-4656-9b0d-7288401bc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def palabras_importantes(documento_id, M=10):\n",
    "    \n",
    "    doc_id_str = str(documento_id)\n",
    "    \n",
    "    # Convertir ID num√©rico a original\n",
    "    if doc_id_str.isdigit():\n",
    "        num_id = int(doc_id_str)\n",
    "        if num_id in numeric_id_to_original:\n",
    "            doc_id_str = numeric_id_to_original[num_id]\n",
    "    \n",
    "    # Buscar documento\n",
    "    row = df.filter(col(\"book_id\") == doc_id_str).select(\"tfidf_norm\").collect()\n",
    "    \n",
    "    if not row:\n",
    "        raise ValueError(f\" Documento '{documento_id}' no encontrado\")\n",
    "    \n",
    "    # Resto del c√≥digo igual...\n",
    "    tfidf_vector = row[0][\"tfidf_norm\"]\n",
    "    vocab = cv_model.vocabulary\n",
    "    \n",
    "    items = list(zip(tfidf_vector.indices, tfidf_vector.values))\n",
    "    items_sorted = sorted(items, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [\n",
    "        (vocab[idx], float(val))\n",
    "        for idx, val in items_sorted[:M]\n",
    "    ]\n",
    "    \n",
    "    return top_words\n",
    "\n",
    "\n",
    "print(\"‚úì Funci√≥n palabras_importantes() definida (versi√≥n normalizada)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc81317-f2f4-4052-9fcc-ef094a0161ea",
   "metadata": {},
   "source": [
    "## INPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b421d4-c9c1-40d2-ae02-eeca25c28827",
   "metadata": {},
   "source": [
    "### CELDA 14: Cat√°logo Completo\n",
    "Muestra los 100 libros disponibles con sus IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862a86e-8d0d-4deb-9b78-d59e31c50bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CAT√ÅLOGO COMPLETO DE LIBROS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal: {len(book_ids)} libros\\n\")\n",
    "\n",
    "for num_id, original_id in numeric_id_to_original.items():\n",
    "    print(f\"{num_id:3}. [ID: {num_id:2}] {id_to_title[original_id]:40} (Original: {original_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cfea0c-626b-4ce0-8026-49461350b8b9",
   "metadata": {},
   "source": [
    "### CELDA 15: Recomendador con INPUT\n",
    "El usuario ingresa:\n",
    "- ID del libro\n",
    "- Cu√°ntas recomendaciones quiere\n",
    "‚Üí Sistema devuelve libros similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbba0f-7667-4759-a63a-27ff1ecb314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "libro_id = input(\"\\nüìñ Ingresa el ID del libro (n√∫mero o nombre): \").strip()\n",
    "n_recs = int(input(\"¬øCu√°ntas recomendaciones?: \") or \"5\")\n",
    "\n",
    "try:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìñ BUSCANDO LIBRO...\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    recomendaciones = recomendar_libros(libro_id, N=n_recs)\n",
    "    \n",
    "    print(f\" TOP {n_recs} RECOMENDACIONES:\\n\")\n",
    "    for i, (num_id, orig_id, title, score) in enumerate(recomendaciones, 1):\n",
    "        print(f\"{i}. [ID: {num_id:2}] [{score:.4f}] {title}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ {n_recs} recomendaciones generadas\")\n",
    "except ValueError as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa848f-df70-4182-be4e-bff94be19ae7",
   "metadata": {},
   "source": [
    "### CELDA 16: Palabras Caracter√≠sticas con INPUT\n",
    "El usuario ingresa:\n",
    "- ID del libro\n",
    "- Cu√°ntas palabras quiere\n",
    "‚Üí Sistema muestra palabras clave con barras visuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585b093-7183-4cf2-a96e-cc4f29827c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 19: Palabras Caracter√≠sticas con INPUT (CORREGIDA)\n",
    "\n",
    "libro_id = input(\"\\n Ingresa el ID del libro (n√∫mero o nombre): \").strip()\n",
    "m_palabras = int(input(\" ¬øCu√°ntas palabras?: \") or \"10\")\n",
    "\n",
    "try:\n",
    "    # Convertir ID num√©rico si es necesario\n",
    "    libro_id_str = str(libro_id)\n",
    "    if libro_id_str.isdigit():\n",
    "        num_id = int(libro_id_str)\n",
    "        if num_id in numeric_id_to_original:\n",
    "            libro_id_str = numeric_id_to_original[num_id]\n",
    "            print(f\"Usando ID num√©rico {num_id} ‚Üí {libro_id_str}\")\n",
    "    else:\n",
    "        libro_id_str = libro_id\n",
    "    \n",
    "    # Validar que existe\n",
    "    if libro_id_str not in id_to_title:\n",
    "        print(f\" Libro '{libro_id}' no encontrado\")\n",
    "    else:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"LIBRO: {id_to_title[libro_id_str]}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        palabras = palabras_importantes(libro_id_str, M=m_palabras)\n",
    "        \n",
    "        for i, (palabra, score) in enumerate(palabras, 1):\n",
    "            # Barra visual\n",
    "            bar = \"‚ñà\" * int((score / palabras[0][1]) * 30)\n",
    "            print(f\"{i:2}. {palabra:20} ‚îÇ {score:7.2f} ‚îÇ {bar}\")\n",
    "        \n",
    "        print(f\"\\n Top {m_palabras} palabras generadas\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438b9b6-fa34-4309-842a-d8a855c7a29e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
