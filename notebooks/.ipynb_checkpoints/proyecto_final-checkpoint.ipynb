{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0aeaea-a76d-407d-b47d-f9397a84ca91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/lib/python312.zip',\n",
       " '/usr/lib/python3.12',\n",
       " '/usr/lib/python3.12/lib-dynload',\n",
       " '',\n",
       " '/home/arturo/venv/lib/python3.12/site-packages',\n",
       " '/home/arturo/project_gutenberg',\n",
       " '/home/arturo/project_gutenberg/notebooks']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Agregar la ruta del proyecto a sys.path\n",
    "sys.path.append(os.path.abspath(\"..\"))       # si el notebook está dentro de /notebooks\n",
    "sys.path.append(os.path.abspath(\".\"))        # si el notebook está en la raíz\n",
    "\n",
    "# Mostrar rutas para confirmar\n",
    "sys.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da050cf-f697-435f-b5a9-f6357882eae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/arturo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/arturo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/arturo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e9024c-198a-459d-a771-b1c45a6924a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/05 22:26:10 WARN Utils: Your hostname, arturo-VirtualBox, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/12/05 22:26:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/05 22:26:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/05 22:26:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ProyectoLibros</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b4e81f190a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, Normalizer\n",
    "import numpy as np\n",
    "import os, json, pickle\n",
    "from src.utils import strip_gutenberg_headers, preprocess_text, read_txt, load_all_books\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProyectoLibros\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059198e6-3d7b-439d-9b7a-a4cd0eeabd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"../data\"  # ajusta la ruta si es necesario\n",
    "books = []\n",
    "for fname in sorted(os.listdir(data_dir)):\n",
    "    if not fname.endswith(\".txt\"):\n",
    "        continue\n",
    "    book_id = os.path.splitext(fname)[0]\n",
    "    path = os.path.join(data_dir, fname)\n",
    "    raw = read_txt(path)\n",
    "    main = strip_gutenberg_headers(raw)\n",
    "    # NOTA: aquí guardamos texto completo y tokens para Spark\n",
    "    tokens = preprocess_text(main, language='english')\n",
    "    books.append((int(book_id) if book_id.isdigit() else book_id, fname, main, tokens))\n",
    "\n",
    "len(books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2ef864-264d-4366-abfe-3c72e6bd35cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 22:43:55 WARN TaskSetManager: Stage 1 contains a task of very large size (10072 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|book_id|   title|                                                        text|                                                      tokens|\n",
      "+-------+--------+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|     11|  11.txt|[Illustration]\\n\\n\\n\\n\\nAlice’s Adventures in Wonderland\\...|[illustration, alice, adventures, wonderland, lewis, carr...|\n",
      "|   1342|1342.txt|[Illustration:\\n\\n                             GEORGE ALL...|[illustration, george, allen, publisher, 156, charing, cr...|\n",
      "|   1661|1661.txt|﻿The Project Gutenberg eBook of The Adventures of Sherloc...|[project, gutenberg, ebook, adventures, sherlock, holmes,...|\n",
      "|   2554|2554.txt|CRIME AND PUNISHMENT\\n\\nBy Fyodor Dostoevsky\\n\\n\\n\\nTrans...|[crime, punishment, fyodor, dostoevsky, translated, const...|\n",
      "+-------+--------+------------------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 4 rows\n"
     ]
    }
   ],
   "source": [
    "# Celda 3: DataFrame Spark con (book_id, title, text, tokens)\n",
    "df = spark.createDataFrame([(b[0], b[1], b[2], b[3]) for b in books],\n",
    "                           schema=[\"book_id\",\"title\",\"text\",\"tokens\"])\n",
    "df.show(4, truncate=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f51fa2b-0887-4023-9d44-e71b74ab5cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 22:44:56 WARN TaskSetManager: Stage 2 contains a task of very large size (10072 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------------------------------------------------+\n",
      "|book_id|   title|                                               tokens_nostop|\n",
      "+-------+--------+------------------------------------------------------------+\n",
      "|     11|  11.txt|[illustration, alice, adventures, wonderland, lewis, carr...|\n",
      "|   1342|1342.txt|[illustration, george, allen, publisher, 156, charing, cr...|\n",
      "|   1661|1661.txt|[project, gutenberg, ebook, adventures, sherlock, holmes,...|\n",
      "+-------+--------+------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# Celda 5: StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_nostop\")\n",
    "# si quieres usar tu propia lista: remover.setStopWords(my_list)\n",
    "df = remover.transform(df)\n",
    "df.select(\"book_id\",\"title\",\"tokens_nostop\").show(3, truncate=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d6ed4e-0730-497f-b7ab-46386e06b504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 22:45:52 WARN TaskSetManager: Stage 3 contains a task of very large size (10072 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/12/05 22:46:02 WARN TaskSetManager: Stage 7 contains a task of very large size (10072 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/12/05 22:46:32 WARN TaskSetManager: Stage 8 contains a task of very large size (10072 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 8:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------------------------------------------------+\n",
      "|book_id|   title|                                                  tfidf_norm|\n",
      "+-------+--------+------------------------------------------------------------+\n",
      "|     11|  11.txt|(16730,[0,1,2,3,4,5,6,7,8,9,11,12,14,15,16,17,18,19,20,21...|\n",
      "|   1342|1342.txt|(16730,[0,1,2,3,4,5,6,7,8,9,10,11,12,14,15,16,17,18,19,20...|\n",
      "|   1661|1661.txt|(16730,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19...|\n",
      "+-------+--------+------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Celda 6: CountVectorizer y IDF\n",
    "vocabSize = 20000\n",
    "cv = CountVectorizer(inputCol=\"tokens_nostop\", outputCol=\"raw_features\", vocabSize=vocabSize, minDF=2.0)\n",
    "cv_model = cv.fit(df)\n",
    "df = cv_model.transform(df)\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf\")\n",
    "idf_model = idf.fit(df)\n",
    "df = idf_model.transform(df)\n",
    "\n",
    "# Normalizar tfidf\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"tfidf_norm\", p=2.0)\n",
    "df = normalizer.transform(df)\n",
    "\n",
    "df.select(\"book_id\",\"title\",\"tfidf_norm\").show(3, truncate=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8639504d-67a8-4495-9a41-a65ec575580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 22:51:25 WARN TaskSetManager: Stage 9 contains a task of very large size (10072 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Celda 7: Convertir vectores de Spark a numpy en memoria\n",
    "rows = df.select(\"book_id\",\"title\",\"tfidf_norm\").collect()\n",
    "book_ids = []\n",
    "id_to_title = {}\n",
    "id_to_vec = {}\n",
    "for r in rows:\n",
    "    bid = r[\"book_id\"]\n",
    "    book_ids.append(bid)\n",
    "    id_to_title[bid] = r[\"title\"]\n",
    "    vec = np.array(r[\"tfidf_norm\"].toArray())\n",
    "    id_to_vec[bid] = vec\n",
    "\n",
    "len(book_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9da961e6-569a-4185-9438-a3299e1a04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 8: Matriz de similitud (dot product porque vectores normalizados)\n",
    "n = len(book_ids)\n",
    "sim_matrix = np.zeros((n, n), dtype=float)\n",
    "for i in range(n):\n",
    "    vi = id_to_vec[book_ids[i]]\n",
    "    for j in range(i, n):\n",
    "        vj = id_to_vec[book_ids[j]]\n",
    "        s = float(np.dot(vi, vj))\n",
    "        sim_matrix[i, j] = s\n",
    "        sim_matrix[j, i] = s\n",
    "\n",
    "# guardar índice -> book_id mapping\n",
    "index_to_id = {i: book_ids[i] for i in range(n)}\n",
    "id_to_index = {book_ids[i]: i for i in range(n)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26709430-88c1-49d0-8a7f-8aab303864db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libro base: 2600 2600.txt\n",
      "Recomendados:\n",
      "2554 2554.txt 0.2064\n",
      "1661 1661.txt 0.0938\n",
      "98 98.txt 0.0849\n",
      "84 84.txt 0.0671\n",
      "1342 1342.txt 0.0331\n"
     ]
    }
   ],
   "source": [
    "# Celda 9: recomendador\n",
    "def recomendar(libro_id, N=5):\n",
    "    if libro_id not in id_to_index:\n",
    "        raise ValueError(\"book_id no encontrado\")\n",
    "    idx = id_to_index[libro_id]\n",
    "    sims = sim_matrix[idx]\n",
    "    pairs = [(index_to_id[i], float(sims[i])) for i in range(len(sims)) if i != idx]\n",
    "    pairs_sorted = sorted(pairs, key=lambda x: x[1], reverse=True)\n",
    "    results = [(pid, id_to_title[pid], score) for pid, score in pairs_sorted[:N]]\n",
    "    return results\n",
    "\n",
    "# Ejemplo (reemplaza por un book_id real):\n",
    "ejemplo_id = book_ids[4]\n",
    "print(\"Libro base:\", ejemplo_id, id_to_title[ejemplo_id])\n",
    "print(\"Recomendados:\")\n",
    "for bid, title, score in recomendar(ejemplo_id, N=5):\n",
    "    print(bid, title, round(score, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2cf0be6-8161-4688-9b22-73a96aa4a740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 23:10:11 WARN TaskSetManager: Stage 15 contains a task of very large size (10072 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 15:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elizabeth', 508.5549974349543), ('jane', 392.38346120733877), ('mrs', 214.57207446389168), ('mr', 162.14192193341822), ('catherine', 133.53132034155934), ('illustration', 128.51854973937606), ('gardiner', 126.03044946063531)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Celda 10: top palabras por documento\n",
    "vocab = cv_model.vocabulary  # lista de tokens\n",
    "\n",
    "def top_palabras(libro_id, M=10):\n",
    "    # buscar fila en df con tfidf (no normalizada — usamos tfidf model output)\n",
    "    row = df.filter(col(\"book_id\") == libro_id).select(\"tfidf\").collect()\n",
    "    if not row:\n",
    "        raise ValueError(\"book_id no encontrado en DataFrame\")\n",
    "    vec = row[0][\"tfidf\"]  # SparseVector\n",
    "    items = list(zip(vec.indices, vec.values))\n",
    "    items_sorted = sorted(items, key=lambda x: x[1], reverse=True)\n",
    "    top = [(vocab[idx], float(val)) for idx, val in items_sorted[:M]]\n",
    "    return top\n",
    "\n",
    "# Ejemplo:\n",
    "print(top_palabras(ejemplo_id, M=7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d97b2cb7-cb3c-401d-8963-998842d48407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 23:13:52 WARN TaskSetManager: Stage 16 contains a task of very large size (10072 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/12/05 23:14:03 WARN TaskSetManager: Stage 17 contains a task of very large size (5477 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|book_id|title   |\n",
      "+-------+--------+\n",
      "|11     |11.txt  |\n",
      "|1342   |1342.txt|\n",
      "|1661   |1661.txt|\n",
      "|2554   |2554.txt|\n",
      "|2600   |2600.txt|\n",
      "|2701   |2701.txt|\n",
      "|3296   |3296.txt|\n",
      "|43     |43.txt  |\n",
      "|84     |84.txt  |\n",
      "|98     |98.txt  |\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"book_id\", \"title\").show(200, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c880b257-296f-4470-a19d-fafb76a84e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 23:14:30 WARN TaskSetManager: Stage 18 contains a task of very large size (10072 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/12/05 23:14:34 WARN TaskSetManager: Stage 19 contains a task of very large size (5477 KiB). The maximum recommended task size is 1000 KiB.\n",
      "IOPub data rate exceeded.                                                       \n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.title.contains(\"1342\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f890bd8-008c-42e2-b36b-dfea775288a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_palabras(libro_id, M=10):\n",
    "    row = df.filter(col(\"book_id\") == libro_id).select(\"tfidf\").collect()\n",
    "    if not row:\n",
    "        raise ValueError(\"book_id no encontrado\")\n",
    "    vec = row[0][\"tfidf\"]  # SparseVector\n",
    "    items = list(zip(vec.indices, vec.values))\n",
    "    items_sorted = sorted(items, key=lambda x: x[1], reverse=True)\n",
    "    vocab = cv_model.vocabulary\n",
    "    top = [(vocab[idx], float(val)) for idx, val in items_sorted[:M]]\n",
    "    return top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b75182e3-bf46-49af-a604-e0b620dcc1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 23:23:57 WARN TaskSetManager: Stage 30 contains a task of very large size (10072 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('elizabeth', 72.53807715351286),\n",
       " ('geneva', 46.774187428689395),\n",
       " ('cottagers', 29.883508634996),\n",
       " ('lake', 27.31322461531896),\n",
       " ('cottage', 23.051241310895918),\n",
       " ('labours', 22.087810730214436),\n",
       " ('fiend', 21.214753124961042)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_palabras(84, 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d124e2d5-341e-4c63-8537-99ebbf886bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/home/arturo/project_gutenberg/notebooks/src/download_books.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python src/download_books.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3a574-0265-4750-89aa-423504f13b59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
