{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c35fc44-b642-4f85-8c8e-1e19e276edac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rutas configuradas:\n",
      "  - /home/arturo/venv/lib/python3.12/site-packages\n",
      "  - /home/arturo/project_gutenberg\n",
      "  - /home/arturo/project_gutenberg/src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Agregar rutas del proyecto\n",
    "sys.path.append(os.path.abspath(\"..\"))  # raÃ­z del proyecto\n",
    "sys.path.append(os.path.abspath(\"../src\"))  # carpeta src\n",
    "\n",
    "# Verificar rutas\n",
    "print(\"Rutas configuradas:\")\n",
    "for path in sys.path[-3:]:\n",
    "    print(f\"  - {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70bf0cce-277a-4c2d-9279-eda87ff104ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando recursos de NLTK...\n",
      "âœ“ NLTK configurado\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(\"Descargando recursos de NLTK...\")\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"âœ“ NLTK configurado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c8eca0-8361-4af2-bd81-0e7727d96a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/06 01:13:43 WARN Utils: Your hostname, arturo-VirtualBox, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/12/06 01:13:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/06 01:13:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Spark inicializado\n",
      "  Version: 4.0.1\n",
      "  Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProyectoFinal_RecomendacionLibros\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reducir logs\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"âœ“ Spark inicializado\")\n",
    "print(f\"  Version: {spark.version}\")\n",
    "print(f\"  Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b18b06b-0ec8-4475-9598-02b1bde9a6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Utilidades importadas\n"
     ]
    }
   ],
   "source": [
    "from src.utils import (\n",
    "    read_txt,\n",
    "    strip_gutenberg_headers,\n",
    "    preprocess_text,\n",
    "    load_all_books\n",
    ")\n",
    "\n",
    "print(\"âœ“ Utilidades importadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94852f96-53a6-4ace-8084-22b7a84095fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Cargando 97 libros desde ../data/\n",
      "  âœ“ Procesados 10/97 libros\n",
      "  âœ“ Procesados 20/97 libros\n",
      "  âœ“ Procesados 30/97 libros\n",
      "  âœ“ Procesados 40/97 libros\n",
      "  âœ“ Procesados 50/97 libros\n",
      "  âœ“ Procesados 60/97 libros\n",
      "  âœ“ Procesados 70/97 libros\n",
      "  âœ“ Procesados 80/97 libros\n",
      "  âœ“ Procesados 90/97 libros\n",
      "âœ… Total de libros cargados exitosamente: 97\n",
      "\n",
      "\n",
      "Resumen de carga:\n",
      "   Total de libros: 97\n",
      "   Ejemplo - ID: 100, Tokens: 498519\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"\n",
    "books = load_all_books(data_dir, max_books=100)\n",
    "\n",
    "print(f\"\\nResumen de carga:\")\n",
    "print(f\"   Total de libros: {len(books)}\")\n",
    "if books:\n",
    "    ejemplo = books[0]\n",
    "    print(f\"   Ejemplo - ID: {ejemplo[0]}, Tokens: {len(ejemplo[3])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03d1f561-be48-42d5-8a81-ece277d9b2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando DataFrame de Spark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 01:14:43 WARN TaskSetManager: Stage 0 contains a task of very large size (63793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ DataFrame creado con 97 documentos\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 01:15:48 WARN TaskSetManager: Stage 3 contains a task of very large size (63793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|book_id|title   |\n",
      "+-------+--------+\n",
      "|100    |100.txt |\n",
      "|103    |103.txt |\n",
      "|105    |105.txt |\n",
      "|1080   |1080.txt|\n",
      "|11     |11.txt  |\n",
      "+-------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "print(\"Creando DataFrame de Spark...\")\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(b[0], b[1], b[2], b[3]) for b in books],\n",
    "    schema=[\"book_id\", \"title\", \"text\", \"tokens\"]\n",
    ")\n",
    "\n",
    "print(f\"âœ“ DataFrame creado con {df.count()} documentos\\n\")\n",
    "df.select(\"book_id\", \"title\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5328e9-32a6-4ca2-b70d-643ad70ff643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Tokens ya procesados (stopwords removidas con NLTK)\n",
      "\n",
      "Ejemplo de tokens limpios:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 01:16:40 WARN TaskSetManager: Stage 4 contains a task of very large size (63793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------+\n",
      "|book_id|                                                tokens_clean|\n",
      "+-------+------------------------------------------------------------+\n",
      "|    100|[complete, works, william, shakespeare, william, shakespe...|\n",
      "|    103|[illustration, around, world, eighty, days, jules, verne,...|\n",
      "|    105|[persuasion, jane, austen, contents, chapter, chapter, ii...|\n",
      "+-------+------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumnRenamed(\"tokens\", \"tokens_clean\")\n",
    "\n",
    "print(\"âœ“ Tokens ya procesados (stopwords removidas con NLTK)\")\n",
    "print(\"\\nEjemplo de tokens limpios:\")\n",
    "df.select(\"book_id\", \"tokens_clean\").show(3, truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3486077-f274-439a-a567-977f14d59f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando vocabulario con CountVectorizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 01:18:05 WARN TaskSetManager: Stage 5 contains a task of very large size (63793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Vocabulario creado\n",
      "  TamaÃ±o del vocabulario: 2000 palabras\n",
      "  Top 10 palabras: ['said', 'one', 'would', 'man', 'de', 'could', 'que', 'like', 'time', 'well']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "print(\"Creando vocabulario con CountVectorizer...\")\n",
    "\n",
    "VOCAB_SIZE = 2000\n",
    "MIN_DF = 2  # Palabra debe aparecer en al menos 2 documentos\n",
    "\n",
    "cv = CountVectorizer(\n",
    "    inputCol=\"tokens_clean\",\n",
    "    outputCol=\"raw_features\",\n",
    "    vocabSize=VOCAB_SIZE,\n",
    "    minDF=MIN_DF\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(df)\n",
    "df = cv_model.transform(df)\n",
    "\n",
    "actual_vocab_size = len(cv_model.vocabulary)\n",
    "print(f\"âœ“ Vocabulario creado\")\n",
    "print(f\"  TamaÃ±o del vocabulario: {actual_vocab_size} palabras\")\n",
    "print(f\"  Top 10 palabras: {cv_model.vocabulary[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf6ce95-2df1-4dd5-9d6e-6aabb0933be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 01:19:25 WARN TaskSetManager: Stage 9 contains a task of very large size (63793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ TF-IDF calculado\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "print(\"\\nCalculando TF-IDF...\")\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf\")\n",
    "idf_model = idf.fit(df)\n",
    "df = idf_model.transform(df)\n",
    "\n",
    "print(\"âœ“ TF-IDF calculado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c7eb27d-3444-424a-b8ad-84529f3f522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando vectores TF-IDF...\n",
      "âœ“ Vectores normalizados\n",
      "\n",
      "DataFrame final:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 01:19:56 WARN TaskSetManager: Stage 10 contains a task of very large size (63793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 10:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------------------------------------------------+\n",
      "|book_id|  title|                                                  tfidf_norm|\n",
      "+-------+-------+------------------------------------------------------------+\n",
      "|    100|100.txt|(2000,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,...|\n",
      "|    103|103.txt|(2000,[0,1,2,3,5,7,8,9,10,11,12,13,14,15,16,17,18,19,20,2...|\n",
      "|    105|105.txt|(2000,[0,1,2,3,5,7,8,9,10,11,12,13,14,15,16,17,18,19,20,2...|\n",
      "+-------+-------+------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "print(\"Normalizando vectores TF-IDF...\")\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"tfidf_norm\", p=2.0)\n",
    "df = normalizer.transform(df)\n",
    "\n",
    "print(\"âœ“ Vectores normalizados\")\n",
    "print(\"\\nDataFrame final:\")\n",
    "df.select(\"book_id\", \"title\", \"tfidf_norm\").show(3, truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6f57e3b-1357-4a11-8d18-bb6e305964da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando matriz de similitud...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 01:20:03 WARN TaskSetManager: Stage 11 contains a task of very large size (63793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Convirtiendo vectores a numpy...\n",
      "  â†’ Calculando similitudes...\n",
      "âœ“ Matriz de similitud creada: (97, 97)\n",
      "  Rango de similitudes: [0.0000, 1.0000]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Creando matriz de similitud...\")\n",
    "\n",
    "# Recolectar datos\n",
    "rows = df.select(\"book_id\", \"title\", \"tfidf_norm\").collect()\n",
    "\n",
    "# Preparar estructuras de datos\n",
    "book_ids = [r[\"book_id\"] for r in rows]\n",
    "id_to_title = {r[\"book_id\"]: r[\"title\"] for r in rows}\n",
    "\n",
    "# Convertir vectores a numpy\n",
    "print(\"  â†’ Convirtiendo vectores a numpy...\")\n",
    "vectors = np.array([r[\"tfidf_norm\"].toArray() for r in rows])\n",
    "\n",
    "# Calcular matriz de similitud (producto punto = cosine similarity)\n",
    "print(\"  â†’ Calculando similitudes...\")\n",
    "sim_matrix = np.dot(vectors, vectors.T)\n",
    "\n",
    "# Crear mapeos\n",
    "index_to_id = {i: book_ids[i] for i in range(len(book_ids))}\n",
    "id_to_index = {book_ids[i]: i for i in range(len(book_ids))}\n",
    "\n",
    "print(f\"âœ“ Matriz de similitud creada: {sim_matrix.shape}\")\n",
    "print(f\"  Rango de similitudes: [{sim_matrix.min():.4f}, {sim_matrix.max():.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "520d6174-1846-44ea-bc9e-1bb6126ce3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendar_libros(libro_id, N=5):\n",
    "    libro_id = str(libro_id)  # <-- soluciÃ³n\n",
    "    \n",
    "    if libro_id not in id_to_index:\n",
    "        raise ValueError(f\"âŒ Libro '{libro_id}' no encontrado\")\n",
    "    \n",
    "    idx = id_to_index[libro_id]\n",
    "    similarities = sim_matrix[idx]\n",
    "\n",
    "    pairs = [\n",
    "        (index_to_id[i], float(similarities[i]))\n",
    "        for i in range(len(similarities))\n",
    "        if i != idx\n",
    "    ]\n",
    "\n",
    "    pairs_sorted = sorted(pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results = [\n",
    "        (bid, id_to_title[bid], score)\n",
    "        for bid, score in pairs_sorted[:N]\n",
    "    ]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73bcd5e7-8fd4-4d48-9da9-5f17564c3668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1112', '1112.txt', 0.7441848048758577),\n",
       " ('58', '58.txt', 0.7253547529834251),\n",
       " ('3296', '3296.txt', 0.7028455377826147),\n",
       " ('2265', '2265.txt', 0.652600381541398),\n",
       " ('1322', '1322.txt', 0.5827439799498586)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomendar_libros(100, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72d6bdb0-6247-4656-9b0d-7288401bc221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ FunciÃ³n palabras_importantes() definida\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def palabras_importantes(documento_id, M=10):\n",
    "    \"\"\"\n",
    "    ðŸŽ¯ FUNCIÃ“N PRINCIPAL #2\n",
    "    \n",
    "    Regresa M palabras que describen un documento.\n",
    "    \n",
    "    ParÃ¡metros:\n",
    "        documento_id: ID del documento (book_id)\n",
    "        M: NÃºmero de palabras a retornar\n",
    "    \n",
    "    Retorna:\n",
    "        Lista de tuplas (palabra, tf-idf_score)\n",
    "    \"\"\"\n",
    "    # Buscar documento\n",
    "    row = df.filter(col(\"book_id\") == documento_id).select(\"tfidf\").collect()\n",
    "    \n",
    "    if not row:\n",
    "        raise ValueError(f\"âŒ Documento '{documento_id}' no encontrado\")\n",
    "    \n",
    "    # Obtener vector TF-IDF\n",
    "    tfidf_vector = row[0][\"tfidf\"]\n",
    "    vocab = cv_model.vocabulary\n",
    "    \n",
    "    # Ordenar por score TF-IDF\n",
    "    items = list(zip(tfidf_vector.indices, tfidf_vector.values))\n",
    "    items_sorted = sorted(items, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Convertir a palabras\n",
    "    top_words = [\n",
    "        (vocab[idx], float(val))\n",
    "        for idx, val in items_sorted[:M]\n",
    "    ]\n",
    "    \n",
    "    return top_words\n",
    "\n",
    "\n",
    "print(\"âœ“ FunciÃ³n palabras_importantes() definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0588d64-1e41-42ed-b214-f4d798d6898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 01:20:37 WARN TaskSetManager: Stage 12 contains a task of very large size (63793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('elizabeth', 129.00312038398658),\n",
       " ('feelings', 24.493857723823492),\n",
       " ('william', 21.192334148441287),\n",
       " ('misery', 19.36936799520666),\n",
       " ('henry', 18.151825972355446)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras_importantes(84, M=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccc1da69-1adf-47c3-9431-f53d1fef267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ” EJEMPLO 1: SISTEMA DE RECOMENDACIÃ“N\n",
      "======================================================================\n",
      "\n",
      "ðŸ“– Libro base:\n",
      "   ID: 84\n",
      "   TÃ­tulo: 84.txt\n",
      "\n",
      "ðŸŽ¯ Top 5 libros recomendados:\n",
      "----------------------------------------------------------------------\n",
      "1. [0.6763] 1342.txt\n",
      "   ID: 1342\n",
      "\n",
      "2. [0.4039] 2148.txt\n",
      "   ID: 2148\n",
      "\n",
      "3. [0.3877] 4085.txt\n",
      "   ID: 4085\n",
      "\n",
      "4. [0.3120] 205.txt\n",
      "   ID: 205\n",
      "\n",
      "5. [0.2673] 209.txt\n",
      "   ID: 209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ” EJEMPLO 1: SISTEMA DE RECOMENDACIÃ“N\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Escoger un libro de ejemplo (puedes cambiar el Ã­ndice)\n",
    "ejemplo_libro_id = \"84\"  # Primer libro\n",
    "\n",
    "print(f\"\\nðŸ“– Libro base:\")\n",
    "print(f\"   ID: {ejemplo_libro_id}\")\n",
    "print(f\"   TÃ­tulo: {id_to_title[ejemplo_libro_id]}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Top 5 libros recomendados:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "recomendaciones = recomendar_libros(ejemplo_libro_id, N=5)\n",
    "\n",
    "for i, (bid, title, score) in enumerate(recomendaciones, 1):\n",
    "    print(f\"{i}. [{score:.4f}] {title}\")\n",
    "    print(f\"   ID: {bid}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1775a151-6bf4-4d60-8473-7f7db5384dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸŽ® MODO INTERACTIVO\n",
      "======================================================================\n",
      "\n",
      "ðŸ’¡ Usa la funciÃ³n analizar_libro() para explorar:\n",
      "   analizar_libro(\"1342\", n_recomendaciones=5, m_palabras=10)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ® MODO INTERACTIVO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# FunciÃ³n helper para anÃ¡lisis rÃ¡pido\n",
    "def analizar_libro(libro_id, n_recomendaciones=5, m_palabras=10):\n",
    "    \"\"\"FunciÃ³n de ayuda para anÃ¡lisis rÃ¡pido\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ“Š ANÃLISIS DEL LIBRO: {libro_id}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“– InformaciÃ³n:\")\n",
    "    print(f\"   TÃ­tulo: {id_to_title[libro_id]}\")\n",
    "    \n",
    "    # Palabras caracterÃ­sticas\n",
    "    print(f\"\\nðŸ“ Top {m_palabras} palabras caracterÃ­sticas:\")\n",
    "    palabras = palabras_importantes(libro_id, M=m_palabras)\n",
    "    for i, (palabra, score) in enumerate(palabras, 1):\n",
    "        print(f\"   {i:2}. {palabra:15} (TF-IDF: {score:.2f})\")\n",
    "    \n",
    "    # Recomendaciones\n",
    "    print(f\"\\nðŸŽ¯ Top {n_recomendaciones} libros similares:\")\n",
    "    recs = recomendar_libros(libro_id, N=n_recomendaciones)\n",
    "    for i, (bid, title, score) in enumerate(recs, 1):\n",
    "        print(f\"   {i}. [{score:.4f}] {title}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "print(\"\\nðŸ’¡ Usa la funciÃ³n analizar_libro() para explorar:\")\n",
    "print('   analizar_libro(\"1342\", n_recomendaciones=5, m_palabras=10)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f06837b-b59a-4187-a294-da078d7614ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“Š ANÃLISIS DEL LIBRO: 84\n",
      "======================================================================\n",
      "\n",
      "ðŸ“– InformaciÃ³n:\n",
      "   TÃ­tulo: 84.txt\n",
      "\n",
      "ðŸ“ Top 10 palabras caracterÃ­sticas:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 01:27:17 WARN TaskSetManager: Stage 13 contains a task of very large size (63793 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1. elizabeth       (TF-IDF: 129.00)\n",
      "    2. feelings        (TF-IDF: 24.49)\n",
      "    3. william         (TF-IDF: 21.19)\n",
      "    4. misery          (TF-IDF: 19.37)\n",
      "    5. henry           (TF-IDF: 18.15)\n",
      "    6. chapter         (TF-IDF: 17.54)\n",
      "    7. beheld          (TF-IDF: 17.35)\n",
      "    8. countenance     (TF-IDF: 16.19)\n",
      "    9. science         (TF-IDF: 15.67)\n",
      "   10. towards         (TF-IDF: 15.62)\n",
      "\n",
      "ðŸŽ¯ Top 5 libros similares:\n",
      "   1. [0.6763] 1342.txt\n",
      "   2. [0.4039] 2148.txt\n",
      "   3. [0.3877] 4085.txt\n",
      "   4. [0.3120] 205.txt\n",
      "   5. [0.2673] 209.txt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "analizar_libro(\"84\", n_recomendaciones=5, m_palabras=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0b26953-f380-45a7-83ae-fe7d02c3184e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862a86e-8d0d-4deb-9b78-d59e31c50bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
