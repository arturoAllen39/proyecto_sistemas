{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36761ab-27e0-436e-9364-d5d78c9de6aa",
   "metadata": {},
   "source": [
    "# Aqui inicia mi sufrimiento :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0734abf6-fdf5-41bb-897d-7e830b7a703f",
   "metadata": {},
   "source": [
    "# PROYECTO FINAL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f105ba-bc9e-4ecc-8cc6-31f48eb3973d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sistemas Distribuidos con PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6529ba-82b3-4d45-ad34-217567980814",
   "metadata": {},
   "source": [
    "Este proyecto implementa lo que se ha visto en clases en una serie de ejercicios a las cuales llamo un **sistema de recomendaci√≥n de libros** utilizando:\n",
    "- **100 libros** m√°s descargados de Project Gutenberg (Omitiendo 2 ya que estos denegaban el acceso...)\n",
    "- **PySpark** para procesamiento distribuido\n",
    "- **TF-IDF** para an√°lisis de texto\n",
    "- **Similitud coseno** para encontrar libros similares\n",
    "- Y las ense√±anzas del profe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ab2e8-c827-4e56-8630-6e32b7433b30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CELDA 1: Configuraci√≥n de Rutas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1dc7cd-9fd2-4c4b-ad2f-5d65dcde75cd",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\n",
    "Configura las rutas del proyecto para que Python pueda encontrar nuestros archivos de utilidades (`src/utils.py`).\n",
    "\n",
    "**Explicaci√≥n t√©cnica:**\n",
    "- `sys.path.append()` a√±ade directorios donde Python buscar√° m√≥dulos <details> <summary> ¬øPor que? </summary> (Antes tuvimos problemas con que el programa que no encontraba la ruta de SRC)</details>\n",
    "\n",
    "\n",
    "- `..` = directorio padre (ra√≠z del proyecto)\n",
    "- `../src` = carpeta de c√≥digo fuente<details>\n",
    "    <summary> ¬øQue hay ahi? </summary> \n",
    "    Ahi adentro se encuentran los archivos python donde uno descarga los libros (download_books.py) y otro donde se encarga de limpiar estos mismos como quitar los headers o footers para que no extorben en el analisis (utils.py)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433a5e47-9932-438f-a8bf-4394212638ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rutas configuradas:\n",
      "  - /home/arturo/venv/lib/python3.12/site-packages\n",
      "  - /home/arturo/project_gutenberg\n",
      "  - /home/arturo/project_gutenberg/src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Agregar rutas del proyecto\n",
    "sys.path.append(os.path.abspath(\"..\"))  # ra√≠z del proyecto\n",
    "sys.path.append(os.path.abspath(\"../src\"))  # carpeta src\n",
    "\n",
    "# Verificar rutas\n",
    "print(\"Rutas configuradas:\")\n",
    "for path in sys.path[-3:]:\n",
    "    print(f\"  - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6cbe4-d551-414c-b1cd-d83fe5084d05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CELDA 2: Descargar Recursos de NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78bc74a-185b-458b-bae2-0cec356ead26",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\n",
    "Descarga las herramientas necesarias de NLTK para procesar texto en ingl√©s.\n",
    "\n",
    "**Lo que descarga:**\n",
    "- `punkt` y `punkt_tab` ‚Üí Separa texto en palabras (tokenizaci√≥n)<details><summary>En otras palabras</summary>Esto divide un texto grande en piezas m√°s peque√±as llamadas tokens.</details>\n",
    "\n",
    "- `stopwords` ‚Üí Lista de palabras comunes que no aportan significado (\"the\", \"a\", \"is\") <details>\n",
    "  <summary> ¬øDe donde sacamos esta herramienta? </summary>\n",
    "  Esta herramienta o mas bien libreria fue recomendada por el profesor para facilitarnos la detecci√≤n de stopwords\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70bf0cce-277a-4c2d-9279-eda87ff104ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando recursos de NLTK...\n",
      "‚úì NLTK configurado\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(\"Descargando recursos de NLTK...\")\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"‚úì NLTK configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9da3f-8de6-4cd3-a39d-0c26d6526d31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CELDA 3: Inicializar Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ae35a-8dd2-4c83-b2b9-6ad323492441",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\n",
    "Arranca el motor de Spark que procesar√° nuestros datos de forma distribuida.\n",
    "\n",
    "**Configuraci√≥n:**\n",
    "- `driver.memory: 6g` ‚Üí Memoria del coordinador (6 GB)<details><summary>Mas detalles</summary>Memoria para el driver.\n",
    "    - Este driver.memory recibe las instrucciones, reparte el trabajo y al final recoge los resultados.\n",
    "    - Los 6 GB significa que la m√°quina virtual va a reservar 6 GB solo para este coordinador.\n",
    "    - Esto procesara los 100 libros\n",
    "</details>\n",
    "\n",
    "- `executor.memory: 4g` ‚Üí Memoria de los trabajadores (4 GB)\n",
    "- `local[*]` ‚Üí Usa todos los cores de tu CPU\n",
    "\n",
    "\\\n",
    "**¬øPor qu√© Spark?**\n",
    "- Para procesar 100 libros (~75 MB) de forma eficiente y paralela.<details>\n",
    "    <summary> Y... </summary> \n",
    "    ...porque el profe nos lo pidio\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c8eca0-8361-4af2-bd81-0e7727d96a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/07 23:28:24 WARN Utils: Your hostname, arturo-VirtualBox, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/12/07 23:28:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 23:28:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Spark inicializado\n",
      "  Version: 4.0.1\n",
      "  Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProyectoFinal_RecomendacionLibros\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reducir logs\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úì Spark inicializado\")\n",
    "print(f\"  Version: {spark.version}\")\n",
    "print(f\"  Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b85d2313-1fcb-4619-9227-8a5f50c345c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CELDA 4: Importar Funciones de Utilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69880f-56d6-48b3-ac38-117eeadca922",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**¬øQu√© hace?**\\\n",
    "Carga las funciones que creamos para procesar los libros desde la carpeta src al archivo utils.py\n",
    "\n",
    "**Funciones importadas:**\n",
    "- `read_txt()` ‚Üí Lee un archivo .txt sin importar el tipo de encoding, evitando errores por acentos o caracteres raros.\n",
    "- `strip_gutenberg_headers()` ‚Üí Quita las licencias, advertencias y texto extra que agregan los libros.\n",
    "- `preprocess_text()` ‚Üí Limpia el texto aplicando estos filtros:\n",
    "    - min√∫sculas\n",
    "    - tokenizar\n",
    "    - quitar stopwords\n",
    "    - quitar puntuaci√≥n<br>\n",
    "- `load_all_books()` ‚Üí Busca todos los .txt de la carpeta y:\\\n",
    "  Es la funci√≥n que carga todos los libros del proyecto.\n",
    "    - los lee\n",
    "    - los limpia\n",
    "    - los convierte en una lista/diccionario de libros procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b18b06b-0ec8-4475-9598-02b1bde9a6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Utilidades importadas\n"
     ]
    }
   ],
   "source": [
    "from src.utils import (\n",
    "    read_txt,\n",
    "    strip_gutenberg_headers,\n",
    "    preprocess_text,\n",
    "    load_all_books\n",
    ")\n",
    "\n",
    "print(\"‚úì Utilidades importadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da20a93-d253-4d3b-988d-65a24569b75e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CELDA 5 y 6: Cargar 100 Libros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2af80-4949-415e-8b79-2ea4dd9db477",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\n",
    "Lee los 100 lirbos .txt desde la carpeta `data/` y los preprocesa.\n",
    "\n",
    "**¬øEn que consiste el proceso?**\n",
    "1. Lee el archivo\n",
    "2. Elimina headers/footers\n",
    "3. Convierte a min√∫sculas\n",
    "4. Separa en palabras (tokens)\n",
    "5. Elimina stopwords (\"the\", \"a\", etc.)\n",
    "6. Elimina puntuaci√≥n y n√∫meros\n",
    "\n",
    "**Resultado:**\n",
    "Una lista con 100 libros, cada uno con:\n",
    "- ID del libro (usando los ID originales de la pagina)\n",
    "- Nombre del archivo\n",
    "- Texto completo\n",
    "- Lista de palabras limpias (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b046493d-9de7-48ae-905f-d889e8f27cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desmarcar el comentario en caso de extrema emergencia (No tener los libros)\n",
    "# python3 src/download_books.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5548773-4fb8-4791-8945-456cf428e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Cargando 100 libros desde ../data/\n",
      "  ‚úì Procesados 10/100 libros\n",
      "  ‚úì Procesados 20/100 libros\n",
      "  ‚úì Procesados 30/100 libros\n",
      "  ‚úì Procesados 40/100 libros\n",
      "  ‚úì Procesados 50/100 libros\n",
      "  ‚úì Procesados 60/100 libros\n",
      "  ‚úì Procesados 70/100 libros\n",
      "  ‚úì Procesados 80/100 libros\n",
      "  ‚úì Procesados 90/100 libros\n",
      "  ‚úì Procesados 100/100 libros\n",
      "‚úÖ Total de libros cargados exitosamente: 100\n",
      "\n",
      "\n",
      "üìä Resumen de carga:\n",
      "   Total de libros: 100\n",
      "   Ejemplo - ID: 100, Tokens: 498519\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"\n",
    "books = load_all_books(data_dir, max_books=100)\n",
    "\n",
    "print(f\"\\nüìä Resumen de carga:\")\n",
    "print(f\"   Total de libros: {len(books)}\")\n",
    "if books:\n",
    "    ejemplo = books[0]\n",
    "    print(f\"   Ejemplo - ID: {ejemplo[0]}, Tokens: {len(ejemplo[3])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e19a27-a5bf-4c65-8776-cb644ba39a9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CELDA 7: Crear DataFrame de Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a187e94-7db3-46c0-9c2f-01aa1fb5d8ff",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\n",
    "Convierte nuestra lista de libros en un **DataFrame de Spark** (como una especie de tabla de Excel gigante).\n",
    "\n",
    "**Columnas del DataFrame:**\n",
    "- `book_id` ‚Üí ID √∫nico del libro (ej: \"84\", \"1342\")\n",
    "- `title` ‚Üí Nombre del archivo (ej: \"84.txt\")\n",
    "- `text` ‚Üí Texto completo del libro\n",
    "- `tokens` ‚Üí Lista de palabras procesadas\n",
    "\n",
    "**¬øPor qu√© un DataFrame?**\n",
    "Spark puede procesar DataFrames de forma paralela y distribuida.\n",
    "<details><summary>Comentario</summary> Creemos que no se ve las columnas de \"text\" y \"tokens\" porque estamos usando una maquina virutal para correr este codigo y la verdad a este punto en el que estoy hubiera preferido instalar linux pero en la laptop, casi no la uso</details>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d1f561-be48-42d5-8a81-ece277d9b2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando DataFrame de Spark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 23:29:49 WARN TaskSetManager: Stage 0 contains a task of very large size (74586 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì DataFrame creado con 100 documentos\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 23:31:38 WARN TaskSetManager: Stage 3 contains a task of very large size (74586 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|book_id|title    |\n",
      "+-------+---------+\n",
      "|100    |100.txt  |\n",
      "|1023   |1023.txt |\n",
      "|10554  |10554.txt|\n",
      "|1080   |1080.txt |\n",
      "|11     |11.txt   |\n",
      "+-------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "print(\"Creando DataFrame de Spark...\")\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(b[0], b[1], b[2], b[3]) for b in books],\n",
    "    schema=[\"book_id\", \"title\", \"text\", \"tokens\"]\n",
    ")\n",
    "\n",
    "print(f\"‚úì DataFrame creado con {df.count()} documentos\\n\")\n",
    "df.select(\"book_id\", \"title\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf6c82-765e-4931-a516-c8d5fe2d7d20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CELDA 8: Renombrar Columna (IGNORAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b30ce-ff4b-42cc-b9df-93f10b28d506",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\\\n",
    "Renombra la columna `tokens` a `tokens_clean` para claridad.\n",
    "\n",
    "**¬øPor qu√©?**\\\n",
    "Al inicio lo dejamos para ver si nos daba los tokens y TODO el texto del libro pero al final lo dejamos asi porque nos dio miedo que se rompiera el codigo y tener que esperar otros 15 minutos a que corriera esto en la maquina virutal, asi que lo dejamos como algo para ignorar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b5328e9-32a6-4ca2-b70d-643ad70ff643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tokens ya procesados (stopwords removidas con NLTK)\n",
      "\n",
      "Ejemplo de tokens limpios:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 23:32:38 WARN TaskSetManager: Stage 4 contains a task of very large size (74586 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------+\n",
      "|book_id|                                                tokens_clean|\n",
      "+-------+------------------------------------------------------------+\n",
      "|    100|[complete, works, william, shakespeare, william, shakespe...|\n",
      "|   1023|[bleak, house, charles, dickens, contents, preface, chanc...|\n",
      "|  10554|[right, ho, jeeves, wodehouse, raymond, needham, affectio...|\n",
      "+-------+------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumnRenamed(\"tokens\", \"tokens_clean\")\n",
    "\n",
    "print(\"‚úì Tokens ya procesados (stopwords removidas con NLTK)\")\n",
    "print(\"\\nEjemplo de tokens limpios:\")\n",
    "df.select(\"book_id\", \"tokens_clean\").show(3, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7075463e-47af-47da-b461-7c92cfad75c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CELDA 9: Crear Vocabulario (CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c0a3d-f909-49de-bd0a-36601592ee25",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\\\n",
    "Crea un **vocabulario** con las 2000 palabras m√°s importantes de todos los libros.\n",
    "\n",
    "**Proceso:**\n",
    "1. Cuenta cu√°ntas veces aparece cada palabra en cada libro\n",
    "2. Selecciona las 2000 palabras m√°s frecuentes\n",
    "3. Filtra palabras que aparecen en menos de 2 libros (muy raras)\n",
    "\n",
    "**Resultado:**\\\n",
    "Cada libro se representa como un vector de 2000 n√∫meros (frecuencias de palabras).\n",
    "<details>\n",
    "  <summary>Detalles del codigo</summary>\n",
    "  <p>Par√°metros:</p>\n",
    "  <ul>\n",
    "    <li>VOCAB_SIZE=2000: limita el vocabulario a las 2000 palabras m√°s frecuentes. Evita vectores enormes.</li>\n",
    "    <li>MIN_DF=2: elimina palabras que aparecen en menos de 2 documentos (filtra ruido / typos).</li>\n",
    "  </ul>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3486077-f274-439a-a567-977f14d59f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando vocabulario con CountVectorizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 23:33:44 WARN TaskSetManager: Stage 5 contains a task of very large size (74586 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Vocabulario creado\n",
      "  Tama√±o del vocabulario: 2000 palabras\n",
      "  Top 10 palabras: ['one', 'said', 'would', 'man', 'could', 'upon', 'time', 'good', 'like', 'well']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "print(\"Creando vocabulario con CountVectorizer...\")\n",
    "\n",
    "VOCAB_SIZE = 2000\n",
    "MIN_DF = 2  # Palabra que debe aparecer en al menos 2 documentos\n",
    "\n",
    "cv = CountVectorizer(\n",
    "    inputCol=\"tokens_clean\",\n",
    "    outputCol=\"raw_features\",\n",
    "    vocabSize=VOCAB_SIZE,\n",
    "    minDF=MIN_DF\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(df)\n",
    "df = cv_model.transform(df)\n",
    "\n",
    "actual_vocab_size = len(cv_model.vocabulary)\n",
    "print(f\"‚úì Vocabulario creado\")\n",
    "print(f\"  Tama√±o del vocabulario: {actual_vocab_size} palabras\")\n",
    "print(f\"  Top 10 palabras: {cv_model.vocabulary[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6e2eb-bce7-47e7-9456-a16c03c94e98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CELDA 10: Calcular TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a4951-3252-4492-99dd-a72887e6b499",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\\\n",
    "Calcula **TF-IDF** (Term Frequency - Inverse Document Frequency) para cada palabra.\n",
    "\n",
    "**¬øQu√© es TF-IDF?**\\\n",
    "Un n√∫mero que indica el **peso** o **importancia** en una palabra para un libro espec√≠fico.\n",
    "\n",
    "**F√≥rmula simple:**\n",
    "- **TF** (frecuencia): ¬øCu√°ntas veces aparece en ESTE libro?\n",
    "- **IDF** (rareza): ¬øQu√© tan rara es en TODOS los libros?\n",
    "- **TF-IDF** = TF √ó IDF\n",
    "\n",
    "**Ejemplo:**\n",
    "- \"elizabeth\" aparece mucho en Frankenstein ‚Üí TF alto\n",
    "- \"elizabeth\" NO aparece en otros libros ‚Üí IDF alto\n",
    "- **TF-IDF de \"elizabeth\"** = ALTO (palabra caracter√≠stica)\n",
    "\n",
    "**¬øAl final que se obtiene de todo esto?:**\\\n",
    "Cada libro tiene un vector TF-IDF que representa su contenido √∫nico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edf6ce95-2df1-4dd5-9d6e-6aabb0933be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 23:35:31 WARN TaskSetManager: Stage 9 contains a task of very large size (74586 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì TF-IDF calculado\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "print(\"\\nCalculando TF-IDF...\")\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf\")\n",
    "idf_model = idf.fit(df)\n",
    "df = idf_model.transform(df)\n",
    "\n",
    "print(\"‚úì TF-IDF calculado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27740ee8-562e-4785-94b5-1ff086403a3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CELDA 11: Normalizar Vectores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c83a3-e2eb-4d6e-8fa5-ee7a21dc322d",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\\\n",
    "Normaliza los vectores TF-IDF para que todos tengan la misma \"longitud\" matem√°tica.<details> <summary>Ejemplo</summary> Es como comparar la composici√≥n de dos bebidas sin importar el tama√±o del vaso.</details>\n",
    "\n",
    "**¬øPor qu√© normalizar?**\n",
    "- Libros largos tienen n√∫meros m√°s grandes\n",
    "- Queremos comparar **proporcionalmente**, no por tama√±o\n",
    "- Despu√©s de normalizar, los valores est√°n entre 0 y 1\n",
    "\n",
    "\n",
    "**Resultado:**\\\n",
    "Vectores `tfidf_norm` con valores entre 0 y 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c7eb27d-3444-424a-b8ad-84529f3f522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando vectores TF-IDF...\n",
      "‚úì Vectores normalizados\n",
      "\n",
      "DataFrame final:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 23:38:29 WARN TaskSetManager: Stage 10 contains a task of very large size (74586 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------------------------------------------------+\n",
      "|book_id|    title|                                                  tfidf_norm|\n",
      "+-------+---------+------------------------------------------------------------+\n",
      "|    100|  100.txt|(2000,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,...|\n",
      "|   1023| 1023.txt|(2000,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,...|\n",
      "|  10554|10554.txt|(2000,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,...|\n",
      "|   1080| 1080.txt|(2000,[0,1,2,3,4,5,6,7,8,9,10,11,12,14,15,16,17,18,19,20,...|\n",
      "+-------+---------+------------------------------------------------------------+\n",
      "only showing top 4 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "print(\"Normalizando vectores TF-IDF...\")\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"tfidf_norm\", p=2.0)\n",
    "df = normalizer.transform(df)\n",
    "\n",
    "print(\"‚úì Vectores normalizados\")\n",
    "print(\"\\nDataFrame final:\")\n",
    "df.select(\"book_id\", \"title\", \"tfidf_norm\").show(4, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3fb2fa-be44-44b0-85ce-4d5a4270fa31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CELDA 12: Crear Matriz de Similitud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d714ca8-b9a4-46f0-a2b4-6f3553a5e239",
   "metadata": {},
   "source": [
    "**¬øQu√© hace?**\n",
    "Crea una **matriz 100√ó100** que compara cada libro con todos los dem√°s.\n",
    "\n",
    "**Proceso:**\n",
    "1. Convierte vectores de Spark a NumPy (arrays de Python)\n",
    "2. Calcula el **producto punto** entre todos los pares de libros\n",
    "3. El resultado es la **similitud coseno**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6f57e3b-1357-4a11-8d18-bb6e305964da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando matriz de similitud...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 23:39:22 WARN TaskSetManager: Stage 11 contains a task of very large size (74586 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Üí Convirtiendo vectores a numpy...\n",
      "  ‚Üí Calculando similitudes...\n",
      "‚úì Matriz de similitud creada: (100, 100)\n",
      "  Rango de similitudes: [0.0000, 1.0000]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Creando matriz de similitud...\")\n",
    "\n",
    "# Recolectar datos\n",
    "rows = df.select(\"book_id\", \"title\", \"tfidf_norm\").collect()\n",
    "\n",
    "# Preparar estructuras de datos\n",
    "book_ids = [r[\"book_id\"] for r in rows]\n",
    "id_to_title = {r[\"book_id\"]: r[\"title\"] for r in rows}\n",
    "\n",
    "# Convertir vectores a numpy\n",
    "print(\"  ‚Üí Convirtiendo vectores a numpy...\")\n",
    "vectors = np.array([r[\"tfidf_norm\"].toArray() for r in rows])\n",
    "\n",
    "# Calcular matriz de similitud (producto punto = cosine similarity)\n",
    "print(\"  ‚Üí Calculando similitudes...\")\n",
    "sim_matrix = np.dot(vectors, vectors.T)\n",
    "\n",
    "# Crear mapeos\n",
    "index_to_id = {i: book_ids[i] for i in range(len(book_ids))}\n",
    "id_to_index = {book_ids[i]: i for i in range(len(book_ids))}\n",
    "\n",
    "print(f\"‚úì Matriz de similitud creada: {sim_matrix.shape}\")\n",
    "print(f\"  Rango de similitudes: [{sim_matrix.min():.4f}, {sim_matrix.max():.4f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ec04e-f535-4033-a75d-7495a80c4d2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  CELDA 13 y 14: Recomendar libros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f5e8c0-0931-438c-be2f-b77e9689f014",
   "metadata": {},
   "source": [
    "\n",
    "**¬øQu√© hace?**\\\n",
    "Ingresamos un libro, seguido de ello se encuentra los N libros m√°s similares (Por default y para evitar que la memoria muera pusimos 5).\n",
    "\n",
    "**Pasos:**\n",
    "1. Busca el libro en la matriz de similitud\n",
    "2. Obtiene sus similitudes con todos los dem√°s libros\n",
    "3. Ordena de mayor a menor similitud\n",
    "4. Retorna los top N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "520d6174-1846-44ea-bc9e-1bb6126ce3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendar_libros(libro_id, N=5):\n",
    "    libro_id = str(libro_id)\n",
    "    \n",
    "    if libro_id not in id_to_index:\n",
    "        raise ValueError(f\"‚ùå Libro '{libro_id}' no encontrado\")\n",
    "    \n",
    "    idx = id_to_index[libro_id]\n",
    "    similarities = sim_matrix[idx]\n",
    "\n",
    "    pairs = [\n",
    "        (index_to_id[i], float(similarities[i]))\n",
    "        for i in range(len(similarities))\n",
    "        if i != idx\n",
    "    ]\n",
    "\n",
    "    pairs_sorted = sorted(pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results = [\n",
    "        (bid, id_to_title[bid], score)\n",
    "        for bid, score in pairs_sorted[:N]\n",
    "    ]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73bcd5e7-8fd4-4d48-9da9-5f17564c3668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8800', '8800.txt', 0.7089103711909744),\n",
       " ('26', '26.txt', 0.6649206732336214),\n",
       " ('779', '779.txt', 0.6484633338043468),\n",
       " ('3296', '3296.txt', 0.6249226014288817),\n",
       " ('2680', '2680.txt', 0.5586967707233)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomendar_libros(100, N=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35f962-315d-4ed9-bc54-c9fa0d4aa2e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CELDA 15 y 16: Funci√≥n Palabras Importantes (NORMALIZADA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db84348-ebf1-409e-92c2-61b49f4bb6d8",
   "metadata": {},
   "source": [
    "\n",
    "**¬øQu√© hace?**\\\n",
    "Encuentra las M palabras m√°s caracter√≠sticas de un libro.\n",
    "\n",
    "**Proceso:**\n",
    "1. Obtiene el vector TF-IDF normalizado del libro\n",
    "2. Ordena las palabras por su score TF-IDF\n",
    "3. Retorna las top M palabras\n",
    "\n",
    "**¬øPor qu√© usa `tfidf_norm`?**\n",
    "Los valores est√°n entre 0 y 1, m√°s f√°ciles de interpretar como porcentajes.\n",
    "\n",
    "**Entrada:**\n",
    "- `libro_id` = \"84\"\n",
    "- `M` = 5\n",
    "\n",
    "**Salida:**\n",
    "```\n",
    "elizabeth ‚Üí 0.64 (64% de importancia)\n",
    "feelings  ‚Üí 0.15 (15% de importancia)\n",
    "henry     ‚Üí 0.12 (12% de importancia)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72d6bdb0-6247-4656-9b0d-7288401bc221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Funci√≥n palabras_importantes() definida (versi√≥n normalizada)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def palabras_importantes(documento_id, M=10):\n",
    "    \"\"\"\n",
    "    üéØ FUNCI√ìN PRINCIPAL #2 (VERSI√ìN NORMALIZADA)\n",
    "    \n",
    "    Regresa M palabras que describen un documento.\n",
    "    Usa vectores normalizados (valores entre 0 y 1).\n",
    "    \n",
    "    Par√°metros:\n",
    "        documento_id: ID del documento (book_id)\n",
    "        M: N√∫mero de palabras a retornar\n",
    "    \n",
    "    Retorna:\n",
    "        Lista de tuplas (palabra, score_normalizado)\n",
    "    \"\"\"\n",
    "    # Buscar documento - CAMBIO AQU√ç: tfidf_norm en lugar de tfidf\n",
    "    row = df.filter(col(\"book_id\") == documento_id).select(\"tfidf_norm\").collect()\n",
    "    \n",
    "    if not row:\n",
    "        raise ValueError(f\"‚ùå Documento '{documento_id}' no encontrado\")\n",
    "    \n",
    "    # Obtener vector TF-IDF normalizado\n",
    "    tfidf_vector = row[0][\"tfidf_norm\"]\n",
    "    vocab = cv_model.vocabulary\n",
    "    \n",
    "    # Ordenar por score\n",
    "    items = list(zip(tfidf_vector.indices, tfidf_vector.values))\n",
    "    items_sorted = sorted(items, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Convertir a palabras\n",
    "    top_words = [\n",
    "        (vocab[idx], float(val))\n",
    "        for idx, val in items_sorted[:M]\n",
    "    ]\n",
    "    \n",
    "    return top_words\n",
    "\n",
    "\n",
    "print(\"‚úì Funci√≥n palabras_importantes() definida (versi√≥n normalizada)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0588d64-1e41-42ed-b214-f4d798d6898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 23:42:40 WARN TaskSetManager: Stage 12 contains a task of very large size (74586 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('elizabeth', 0.6382942221796127),\n",
       " ('feelings', 0.14750844941878943),\n",
       " ('henry', 0.12106935995588752),\n",
       " ('misery', 0.11192872548646718),\n",
       " ('william', 0.10978409395308954)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras_importantes(84, M=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc81317-f2f4-4052-9fcc-ef094a0161ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## INPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b421d4-c9c1-40d2-ae02-eeca25c28827",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CELDA 17: Cat√°logo Completo\n",
    "Muestra los 100 libros disponibles con sus IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1862a86e-8d0d-4deb-9b78-d59e31c50bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " CAT√ÅLOGO COMPLETO DE LIBROS\n",
      "================================================================================\n",
      "\n",
      "Total: 100 libros\n",
      "\n",
      "  1. ID: 100    ‚Üí 100.txt\n",
      "  2. ID: 1023   ‚Üí 1023.txt\n",
      "  3. ID: 10554  ‚Üí 10554.txt\n",
      "  4. ID: 1080   ‚Üí 1080.txt\n",
      "  5. ID: 11     ‚Üí 11.txt\n",
      "  6. ID: 110    ‚Üí 110.txt\n",
      "  7. ID: 1184   ‚Üí 1184.txt\n",
      "  8. ID: 120    ‚Üí 120.txt\n",
      "  9. ID: 1232   ‚Üí 1232.txt\n",
      " 10. ID: 1259   ‚Üí 1259.txt\n",
      " 11. ID: 1260   ‚Üí 1260.txt\n",
      " 12. ID: 1342   ‚Üí 1342.txt\n",
      " 13. ID: 135    ‚Üí 135.txt\n",
      " 14. ID: 1399   ‚Üí 1399.txt\n",
      " 15. ID: 1400   ‚Üí 1400.txt\n",
      " 16. ID: 145    ‚Üí 145.txt\n",
      " 17. ID: 1497   ‚Üí 1497.txt\n",
      " 18. ID: 1513   ‚Üí 1513.txt\n",
      " 19. ID: 161    ‚Üí 161.txt\n",
      " 20. ID: 16119  ‚Üí 16119.txt\n",
      " 21. ID: 16328  ‚Üí 16328.txt\n",
      " 22. ID: 16389  ‚Üí 16389.txt\n",
      " 23. ID: 1661   ‚Üí 1661.txt\n",
      " 24. ID: 17135  ‚Üí 17135.txt\n",
      " 25. ID: 17199  ‚Üí 17199.txt\n",
      " 26. ID: 1727   ‚Üí 1727.txt\n",
      " 27. ID: 174    ‚Üí 174.txt\n",
      " 28. ID: 17450  ‚Üí 17450.txt\n",
      " 29. ID: 18035  ‚Üí 18035.txt\n",
      " 30. ID: 1952   ‚Üí 1952.txt\n",
      " 31. ID: 1998   ‚Üí 1998.txt\n",
      " 32. ID: 205    ‚Üí 205.txt\n",
      " 33. ID: 2160   ‚Üí 2160.txt\n",
      " 34. ID: 24029  ‚Üí 24029.txt\n",
      " 35. ID: 244    ‚Üí 244.txt\n",
      " 36. ID: 25162  ‚Üí 25162.txt\n",
      " 37. ID: 25344  ‚Üí 25344.txt\n",
      " 38. ID: 2542   ‚Üí 2542.txt\n",
      " 39. ID: 2554   ‚Üí 2554.txt\n",
      " 40. ID: 2591   ‚Üí 2591.txt\n",
      " 41. ID: 26     ‚Üí 26.txt\n",
      " 42. ID: 2600   ‚Üí 2600.txt\n",
      " 43. ID: 26315  ‚Üí 26315.txt\n",
      " 44. ID: 2680   ‚Üí 2680.txt\n",
      " 45. ID: 2701   ‚Üí 2701.txt\n",
      " 46. ID: 27063  ‚Üí 27063.txt\n",
      " 47. ID: 27827  ‚Üí 27827.txt\n",
      " 48. ID: 28054  ‚Üí 28054.txt\n",
      " 49. ID: 30254  ‚Üí 30254.txt\n",
      " 50. ID: 3206   ‚Üí 3206.txt\n",
      " 51. ID: 3207   ‚Üí 3207.txt\n",
      " 52. ID: 3296   ‚Üí 3296.txt\n",
      " 53. ID: 33170  ‚Üí 33170.txt\n",
      " 54. ID: 33944  ‚Üí 33944.txt\n",
      " 55. ID: 345    ‚Üí 345.txt\n",
      " 56. ID: 34901  ‚Üí 34901.txt\n",
      " 57. ID: 36034  ‚Üí 36034.txt\n",
      " 58. ID: 36965  ‚Üí 36965.txt\n",
      " 59. ID: 37106  ‚Üí 37106.txt\n",
      " 60. ID: 394    ‚Üí 394.txt\n",
      " 61. ID: 408    ‚Üí 408.txt\n",
      " 62. ID: 4085   ‚Üí 4085.txt\n",
      " 63. ID: 40957  ‚Üí 40957.txt\n",
      " 64. ID: 41835  ‚Üí 41835.txt\n",
      " 65. ID: 4200   ‚Üí 4200.txt\n",
      " 66. ID: 43     ‚Üí 43.txt\n",
      " 67. ID: 4300   ‚Üí 4300.txt\n",
      " 68. ID: 4363   ‚Üí 4363.txt\n",
      " 69. ID: 45     ‚Üí 45.txt\n",
      " 70. ID: 46     ‚Üí 46.txt\n",
      " 71. ID: 49266  ‚Üí 49266.txt\n",
      " 72. ID: 514    ‚Üí 514.txt\n",
      " 73. ID: 5197   ‚Üí 5197.txt\n",
      " 74. ID: 5200   ‚Üí 5200.txt\n",
      " 75. ID: 52621  ‚Üí 52621.txt\n",
      " 76. ID: 55     ‚Üí 55.txt\n",
      " 77. ID: 56517  ‚Üí 56517.txt\n",
      " 78. ID: 6130   ‚Üí 6130.txt\n",
      " 79. ID: 64317  ‚Üí 64317.txt\n",
      " 80. ID: 6593   ‚Üí 6593.txt\n",
      " 81. ID: 6761   ‚Üí 6761.txt\n",
      " 82. ID: 67979  ‚Üí 67979.txt\n",
      " 83. ID: 72679  ‚Üí 72679.txt\n",
      " 84. ID: 730    ‚Üí 730.txt\n",
      " 85. ID: 7370   ‚Üí 7370.txt\n",
      " 86. ID: 74     ‚Üí 74.txt\n",
      " 87. ID: 76     ‚Üí 76.txt\n",
      " 88. ID: 768    ‚Üí 768.txt\n",
      " 89. ID: 77405  ‚Üí 77405.txt\n",
      " 90. ID: 77408  ‚Üí 77408.txt\n",
      " 91. ID: 77410  ‚Üí 77410.txt\n",
      " 92. ID: 77411  ‚Üí 77411.txt\n",
      " 93. ID: 779    ‚Üí 779.txt\n",
      " 94. ID: 829    ‚Üí 829.txt\n",
      " 95. ID: 84     ‚Üí 84.txt\n",
      " 96. ID: 844    ‚Üí 844.txt\n",
      " 97. ID: 8492   ‚Üí 8492.txt\n",
      " 98. ID: 8800   ‚Üí 8800.txt\n",
      " 99. ID: 98     ‚Üí 98.txt\n",
      "100. ID: 996    ‚Üí 996.txt\n",
      "\n",
      "üí° Copia el ID del libro que te interese para las siguientes celdas\n"
     ]
    }
   ],
   "source": [
    "# CELDA RESULTADOS 1: Mostrar cat√°logo completo\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CAT√ÅLOGO COMPLETO DE LIBROS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal: {len(book_ids)} libros\\n\")\n",
    "\n",
    "for i, bid in enumerate(book_ids, 1):\n",
    "    print(f\"{i:3}. ID: {bid:6} ‚Üí {id_to_title[bid]}\")\n",
    "\n",
    "print(\"\\nüí° Copia el ID del libro que te interese para las siguientes celdas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cfea0c-626b-4ce0-8026-49461350b8b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CELDA 18: Recomendador con INPUT\n",
    "El usuario ingresa:\n",
    "- ID del libro\n",
    "- Cu√°ntas recomendaciones quiere\n",
    "‚Üí Sistema devuelve libros similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8bbba0f-7667-4759-a63a-27ff1ecb314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Ingresa el ID del libro:  84\n",
      "üî¢ ¬øCu√°ntas recomendaciones?:  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìñ LIBRO BASE: 84.txt\n",
      "================================================================================\n",
      "\n",
      "1. [0.5918] 1342.txt (ID: 1342)\n",
      "2. [0.3883] 36965.txt (ID: 36965)\n",
      "3. [0.3776] 4085.txt (ID: 4085)\n",
      "4. [0.3518] 205.txt (ID: 205)\n",
      "5. [0.3493] 36034.txt (ID: 36034)\n",
      "6. [0.3236] 5197.txt (ID: 5197)\n",
      "\n",
      "‚úÖ 6 recomendaciones generadas\n"
     ]
    }
   ],
   "source": [
    "libro_id = input(\"\\nüìñ Ingresa el ID del libro: \").strip()\n",
    "n_recs = int(input(\"üî¢ ¬øCu√°ntas recomendaciones?: \") or \"5\")\n",
    "\n",
    "if libro_id in id_to_title:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìñ LIBRO BASE: {id_to_title[libro_id]}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    recomendaciones = recomendar_libros(libro_id, N=n_recs)\n",
    "    \n",
    "    for i, (bid, title, score) in enumerate(recomendaciones, 1):\n",
    "        print(f\"{i}. [{score:.4f}] {title} (ID: {bid})\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ {n_recs} recomendaciones generadas\")\n",
    "else:\n",
    "    print(f\"‚ùå Libro '{libro_id}' no encontrado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa848f-df70-4182-be4e-bff94be19ae7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CELDA 19: Palabras Caracter√≠sticas con INPUT\n",
    "El usuario ingresa:\n",
    "- ID del libro\n",
    "- Cu√°ntas palabras quiere\n",
    "‚Üí Sistema muestra palabras clave con barras visuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9585b093-7183-4cf2-a96e-cc4f29827c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Ingresa el ID del libro:  84\n",
      "üî¢ ¬øCu√°ntas palabras?:  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìñ LIBRO: 84.txt\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 23:46:23 WARN TaskSetManager: Stage 13 contains a task of very large size (74586 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. elizabeth            ‚îÇ    0.64 ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      " 2. feelings             ‚îÇ    0.15 ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      " 3. henry                ‚îÇ    0.12 ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      " 4. misery               ‚îÇ    0.11 ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      " 5. william              ‚îÇ    0.11 ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      " 6. miserable            ‚îÇ    0.10 ‚îÇ ‚ñà‚ñà‚ñà‚ñà\n",
      " 7. horror               ‚îÇ    0.10 ‚îÇ ‚ñà‚ñà‚ñà‚ñà\n",
      " 8. beheld               ‚îÇ    0.10 ‚îÇ ‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚úÖ Top 8 palabras generadas\n"
     ]
    }
   ],
   "source": [
    "libro_id = input(\"\\nüìñ Ingresa el ID del libro: \").strip()\n",
    "m_palabras = int(input(\"üî¢ ¬øCu√°ntas palabras?: \") or \"10\")\n",
    "\n",
    "if libro_id in id_to_title:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìñ LIBRO: {id_to_title[libro_id]}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    palabras = palabras_importantes(libro_id, M=m_palabras)\n",
    "    \n",
    "    for i, (palabra, score) in enumerate(palabras, 1):\n",
    "        # Barra visual\n",
    "        bar = \"‚ñà\" * int((score / palabras[0][1]) * 30)\n",
    "        print(f\"{i:2}. {palabra:20} ‚îÇ {score:7.2f} ‚îÇ {bar}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Top {m_palabras} palabras generadas\")\n",
    "else:\n",
    "    print(f\"‚ùå Libro '{libro_id}' no encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a999c-fe2f-4099-9837-f1d043c1dcba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CELDA 20: An√°lisis Completo con INPUT\n",
    "El usuario ingresa:\n",
    "- ID del libro\n",
    "- N√∫mero de recomendaciones\n",
    "- N√∫mero de palabras\n",
    "‚Üí Sistema hace an√°lisis completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41a601dc-fa54-4b4f-814d-e75dfe71a3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ ID del libro:  84\n",
      "üî¢ Recomendaciones (default 5):  \n",
      "üî¢ Palabras clave (default 10):  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'analizar_libro' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m m_palabras = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müî¢ Palabras clave (default 10): \u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m10\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m libro_id \u001b[38;5;129;01min\u001b[39;00m id_to_title:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43manalizar_libro\u001b[49m(libro_id, n_recomendaciones=n_recs, m_palabras=m_palabras)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Libro \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibro_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m no encontrado\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'analizar_libro' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "libro_id = input(\"\\nüìñ ID del libro: \").strip()\n",
    "n_recs = int(input(\"üî¢ Recomendaciones (default 5): \") or \"5\")\n",
    "m_palabras = int(input(\"üî¢ Palabras clave (default 10): \") or \"10\")\n",
    "\n",
    "if libro_id in id_to_title:\n",
    "    analizar_libro(libro_id, n_recomendaciones=n_recs, m_palabras=m_palabras)\n",
    "else:\n",
    "    print(f\"‚ùå Libro '{libro_id}' no encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438b9b6-fa34-4309-842a-d8a855c7a29e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
